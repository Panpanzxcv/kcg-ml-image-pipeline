{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1175ff1c-d68b-443a-938b-0c89fbf4c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '../dataset/'\n",
    "\n",
    "EMB_TYPE = 'text_embeds'\n",
    "\n",
    "# DATASET = 'environmental'\n",
    "# DATASET = 'character'\n",
    "DATASET = 'mech'\n",
    "# DATASET = 'icons'\n",
    "# DATASET = 'waifu'\n",
    "# DATASET = 'propaganda-poster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1632b3ca-eb06-41e0-ab6e-73420429048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--EMB_TYPE\", default=EMB_TYPE, type=str, help=\"EMB_TYPE\")\n",
    "parser.add_argument(\"--DATASET\", default=DATASET, type=str, help=\"DATASET\")\n",
    "\n",
    "\n",
    "try:\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    EMB_TYPE = args.EMB_TYPE\n",
    "    DATASET = args.DATASET\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484c6fa5-fef0-4582-8ff2-8d47917f01e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import msgpack\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f1c8ea-90b6-4969-a065-29b5f1500ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_PATH = f'data/{DATASET}/data.json'\n",
    "\n",
    "WEIGHT_PATH = os.path.join('weight/004', DATASET, f'clip_{EMB_TYPE}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dbd742-66e2-4246-9e93-c276ee54ac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "MODEL_NAME = 'openai/clip-vit-large-patch14'\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained('./input/model/clip/txt_emb_tokenizer', local_files_only=True)\n",
    "transformer = CLIPTextModel.from_pretrained('./input/model/clip/txt_emb_model', local_files_only=True).cuda().eval()\n",
    "\n",
    "def get_prompt_embeds(texts, batch_size):\n",
    "    \n",
    "    def worker(batch):\n",
    "        \n",
    "        batch_encoding = tokenizer(\n",
    "            batch,\n",
    "            truncation=True, max_length=77, return_length=True,\n",
    "            return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tokens = batch_encoding[\"input_ids\"].cuda()\n",
    "            clip_text_opt = transformer(input_ids=tokens, output_hidden_states=True, return_dict=True)\n",
    "        \n",
    "        pooler_output = clip_text_opt.pooler_output.detach().cpu().numpy()\n",
    "        \n",
    "        return pooler_output\n",
    "\n",
    "    pooler_outputs = list()\n",
    "    for i in tqdm(range(0, len(texts), batch_size), leave=False):\n",
    "        pooler_output = worker(texts[i:i+batch_size])\n",
    "        pooler_outputs.append(pooler_output)\n",
    "    pooler_outputs = np.concatenate(pooler_outputs, axis=0)\n",
    "\n",
    "    return pooler_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc34cd1-3e8f-49aa-ab0b-48272dab7c63",
   "metadata": {},
   "source": [
    "# load emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0588ce8-1cad-4f43-b320-7035a814aa0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "js = json.load(open(DATA_PATH))\n",
    "\n",
    "file_paths = list()\n",
    "sample_embeds = list()\n",
    "\n",
    "for info in tqdm(js.values(), total=len(js), leave=False):\n",
    "\n",
    "    file_path = os.path.splitext(info['file_path'].split('_')[0])[0]\n",
    "    file_paths.append(file_path)\n",
    "\n",
    "    if EMB_TYPE == 'image_embeds':\n",
    "    \n",
    "        path = os.path.join(ROOT, 'clip', f'{file_path}_clip.msgpack')\n",
    "        with open(path, 'rb') as f:\n",
    "            mp = msgpack.load(f)\n",
    "        sample_embeds.append(np.array(mp['clip-feature-vector']))\n",
    "\n",
    "    elif EMB_TYPE == 'text_embeds':\n",
    "        sample_embeds.append((info['positive_prompt'], info['negative_prompt']))\n",
    "    elif EMB_TYPE == 'pos_embeds':\n",
    "        sample_embeds.append(info['positive_prompt'])\n",
    "    elif EMB_TYPE == 'neg_embeds':\n",
    "        sample_embeds.append(info['negative_prompt'])\n",
    "\n",
    "file_paths = np.array(file_paths)\n",
    "path_to_index = {file_path: i for i, file_path in enumerate(file_paths)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b479e78-8a74-4cf5-a170-fc31288fd38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EMB_TYPE == 'image_embeds':\n",
    "    sample_embeds = np.concatenate(sample_embeds, axis=0)\n",
    "elif EMB_TYPE == 'text_embeds':\n",
    "    pos_prompts, neg_prompts = zip(*sample_embeds)\n",
    "    sample_embeds = np.concatenate([get_prompt_embeds(pos_prompts, 1024), get_prompt_embeds(neg_prompts, 1024)], axis=1)\n",
    "elif EMB_TYPE == 'pos_embeds':\n",
    "    sample_embeds = get_prompt_embeds(sample_embeds, 1024)\n",
    "elif EMB_TYPE == 'neg_embeds':\n",
    "    sample_embeds = get_prompt_embeds(sample_embeds, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5773745-3567-46e1-8d04-83c58ce19971",
   "metadata": {},
   "source": [
    "# load rank data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0d23bb-b4ec-4968-876a-b9b953924604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paths = sorted(glob.glob(os.path.join(ROOT, 'ranking', DATASET, '*.json')))\n",
    "\n",
    "rank_file_paths = list()\n",
    "rank_pairs = list()\n",
    "\n",
    "for path in tqdm(paths):\n",
    "    js = json.load(open(path))\n",
    "\n",
    "    if js['task'] != 'selection':\n",
    "        continue\n",
    "    \n",
    "    file_path_1 = os.path.splitext(js['image_1_metadata']['file_path'])[0].replace('datasets/', '')\n",
    "    file_path_2 = os.path.splitext(js['image_2_metadata']['file_path'])[0].replace('datasets/', '')\n",
    "    \n",
    "    if (file_path_1 not in path_to_index) or (file_path_2 not in path_to_index):\n",
    "        continue\n",
    "    rank_file_paths.append(path)\n",
    "    rank_pairs.append((file_path_1, file_path_2, js['selected_image_index']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa64249b-1f2e-4b49-8276-b212a025865a",
   "metadata": {},
   "source": [
    "# build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acacb29d-b8eb-48db-ae01-0293e4010e07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rank_pairs = pd.DataFrame(rank_pairs, columns=['image_1', 'image_2', 'selected_image_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01062ce-99c8-46be-b171-903e66a22b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_pairs = [((image_1, image_2) if selected_image_index == 0 else (image_2, image_1)) for image_1, image_2, selected_image_index in rank_pairs.itertuples(index=False, name=None)]\n",
    "ordered_pairs = pd.DataFrame(ordered_pairs, columns=['image_1', 'image_2'])\n",
    "\n",
    "ordered_pairs['index_1'] = ordered_pairs['image_1'].apply(path_to_index.get)\n",
    "ordered_pairs['index_2'] = ordered_pairs['image_2'].apply(path_to_index.get)\n",
    "ordered_pairs['file_path'] = [i.replace(f'{ROOT}ranking/{DATASET}/', f'datasets/{DATASET}/data/ranking/aggregate/') for i in rank_file_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4eb257-225c-4222-9070-21b5512d5cd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## build feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf066c-a6ea-4345-8b14-f828a34d83af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_indices, val_indices = train_test_split(ordered_pairs.index, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecb39ea-fa3e-45b3-93ca-5f8c026b378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list()\n",
    "for index_1, index_2 in ordered_pairs.loc[train_indices, ['index_1', 'index_2']].itertuples(index=False, name=None):\n",
    "    train_data.append(np.stack([sample_embeds[index_1], sample_embeds[index_2]], axis=-1))\n",
    "train_data = np.stack(train_data, axis=0)\n",
    "\n",
    "val_data = list()\n",
    "for index_1, index_2 in ordered_pairs.loc[val_indices, ['index_1', 'index_2']].itertuples(index=False, name=None):\n",
    "    val_data.append(np.stack([sample_embeds[index_1], sample_embeds[index_2]], axis=-1))\n",
    "val_data = np.stack(val_data, axis=0)\n",
    "\n",
    "train_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64b9f52-3fde-4297-bc91-23e6f71b5165",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = torch.tensor(train_data).cuda().float()\n",
    "val_dataset = torch.tensor(val_data).cuda().float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a81a3-7fea-45dd-b631-b613dfd24348",
   "metadata": {
    "tags": []
   },
   "source": [
    "# build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e736cc1a-ab3a-4b97-986b-beb6a41fab29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(train_data.shape[1], 1, bias=True)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0790b38f-8a6a-4606-9cc4-0ef9719920a5",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7155774-b0e0-41ed-b77f-4a03a31e28ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782ee7eb-8c01-48ba-b6c5-e3367299e3fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bces, accs = list(), list()\n",
    "\n",
    "for epoch in tqdm(range(1000)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    label = torch.zeros((train_dataset.shape[0],), device='cuda')\n",
    "    \n",
    "    x = train_dataset\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y0 = model(x[..., 0])\n",
    "    y1 = model(x[..., 1])\n",
    "\n",
    "    y = torch.concat([y0, y1], dim=-1)\n",
    "\n",
    "    # backward\n",
    "\n",
    "    bce = torch.nn.functional.cross_entropy(y, label.long())\n",
    "\n",
    "    acc = (y0 > y1).float().mean()\n",
    "\n",
    "    l1 = torch.norm(model.weight, p=1)\n",
    "\n",
    "    loss = bce + l1 * 1e-3\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # warmup.step()\n",
    "\n",
    "    bces.append(bce.detach().cpu().numpy())\n",
    "    accs.append(acc.detach().cpu().numpy())\n",
    "        \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        val_bces, val_accs = list(), list()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            x = val_dataset\n",
    "\n",
    "            with torch.cuda.amp.autocast(True):\n",
    "\n",
    "                y0 = model(x[..., 0])\n",
    "                y1 = model(x[..., 1])\n",
    "\n",
    "                y = torch.concat([y0, y1], dim=-1)\n",
    "\n",
    "            label = torch.zeros((y.shape[0],), device='cuda').long()\n",
    "\n",
    "            bce = torch.nn.functional.cross_entropy(y, label)\n",
    "\n",
    "            acc = (y.argmax(dim=-1) == 0).float().mean()\n",
    "\n",
    "            val_bces.append(bce.detach().cpu().numpy())\n",
    "            val_accs.append(acc.detach().cpu().numpy())\n",
    "\n",
    "        print(f'{np.mean(bces):.4f} {np.mean(accs):.4f} {np.mean(val_bces):.4f} {np.mean(val_accs):.4f}')\n",
    "    \n",
    "        bces, accs = list(), list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf1412e-5c2a-4f24-b392-bfeb5e44b728",
   "metadata": {},
   "source": [
    "## calculate score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9efd92-426f-453b-82e5-43523b389fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(True):\n",
    "        score = model(torch.tensor(sample_embeds).half().cuda())[:, 0]\n",
    "        score = score.detach().cpu().numpy().astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806c31fa-ff7e-4901-b047-52906fa6d37d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score_mean, score_std = score.mean(axis=0), score.std(axis=0)\n",
    "sigma_score = (score - score_mean[None]) / score_std[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff79d5d8-a3af-44be-bb0f-0b9477dbadb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_pairs['sigma_score_1'] = sigma_score[ordered_pairs['index_1']]\n",
    "ordered_pairs['sigma_score_2'] = sigma_score[ordered_pairs['index_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3248dbe7-9e7f-44be-8e67-652be8c90392",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(12, 4))\n",
    "\n",
    "pyplot.subplot(1, 2, 1)\n",
    "\n",
    "_ = pyplot.hist(score, bins=100, density=True)\n",
    "\n",
    "pyplot.subplot(1, 2, 2)\n",
    "\n",
    "_ = pyplot.hist(ordered_pairs['sigma_score_1'].values, bins=100, density=True, alpha=0.5, color='r')\n",
    "_ = pyplot.hist(ordered_pairs['sigma_score_2'].values, bins=100, density=True, alpha=0.5, color='b')\n",
    "pyplot.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5935c876-24ae-467f-8cb7-e482a0270357",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aa1c79-3dd6-4f2a-a5c6-e19084136f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.split(WEIGHT_PATH)[0], exist_ok=True)\n",
    "torch.save(model.state_dict(), WEIGHT_PATH)\n",
    "np.savez(\n",
    "    WEIGHT_PATH.replace('.pt', '.npz'), \n",
    "    mean=score_mean,\n",
    "    std=score_std,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bbb089-f021-48bf-8749-ec0ee30f9ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559fdffe-0192-4900-8321-a110859b450d",
   "metadata": {},
   "source": [
    "# check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da3b4df-0db1-4981-a117-57ef605f9cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINIO_ADDRESS = \"123.176.98.90:9000\"\n",
    "MINIO_ADDRESS = \"192.168.3.5:9000\"\n",
    "access_key = \"GXvqLWtthELCaROPITOG\"\n",
    "secret_key = \"DmlKgey5u0DnMHP30Vg7rkLT0NNbNIGaM8IwPckD\"\n",
    "bucket_name = 'datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa44e0e5-4997-4f54-a799-fc13cc87ba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../kcg-ml-image-pipeline/'))\n",
    "\n",
    "from utility.minio.cmd import connect_to_minio_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecc8ebe-378c-4cfd-a9a8-cd2b9446346f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = connect_to_minio_client(MINIO_ADDRESS, access_key, secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2bdcdd-bd0e-4ec6-8588-25ddc617a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_by_path(file_path):\n",
    "    \n",
    "    data = client.get_object(bucket_name=bucket_name, object_name=f'{file_path}.jpg')\n",
    "\n",
    "    return Image.open(BytesIO(data.data))\n",
    "\n",
    "def show_images(file_paths):\n",
    "\n",
    "    num_rows = max(1, int(np.floor(len(file_paths) ** 0.5)))\n",
    "    file_paths = file_paths[:num_rows * num_rows]\n",
    "\n",
    "    target_size = 1024 // num_rows\n",
    "\n",
    "    images = list()\n",
    "    for file_path in tqdm(file_paths, leave=False):\n",
    "        img = get_image_by_path(file_path)\n",
    "        images.append(np.array(img.resize((target_size, target_size))))\n",
    "\n",
    "    images = np.stack(images)\n",
    "    images = images.reshape(num_rows, num_rows, target_size, target_size, 3)\n",
    "    images = np.concatenate(np.concatenate(images, axis=-3), axis=-2)\n",
    "    return Image.fromarray(images)\n",
    "\n",
    "def show_pairs(file_paths_1, file_paths_2):\n",
    "\n",
    "    n = len(file_paths_1)\n",
    "\n",
    "    target_size = 1024 // n\n",
    "\n",
    "    images_1 = list()\n",
    "    for file_path in tqdm(file_paths_1, leave=False):\n",
    "        img = get_image_by_path(file_path)\n",
    "        images_1.append(np.array(img.resize((target_size, target_size))))\n",
    "\n",
    "    images_1 = np.stack(images_1)\n",
    "    images_1 = np.concatenate(images_1, axis=-2)\n",
    "\n",
    "    images_2 = list()\n",
    "    for file_path in tqdm(file_paths_2, leave=False):\n",
    "        img = get_image_by_path(file_path)\n",
    "        images_2.append(np.array(img.resize((target_size, target_size))))\n",
    "\n",
    "    images_2 = np.stack(images_2)\n",
    "    images_2 = np.concatenate(images_2, axis=-2)\n",
    "\n",
    "    images = np.concatenate([images_1, images_2], axis=-3)\n",
    "    \n",
    "    return Image.fromarray(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a56b8-646d-4311-b89d-c2036ecfda3b",
   "metadata": {},
   "source": [
    "## check pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706db31b-66fd-463f-aeab-53473093559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = ordered_pairs.query('(sigma_score_1 - sigma_score_2 < -2) and sigma_score_2 > 1')\n",
    "selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edfa5f4-7730-4883-aa47-6a8266c990dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_pairs(selected['image_1'][:8], selected['image_2'][:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852d0257-87ac-40e4-b246-b9eec420c79c",
   "metadata": {},
   "source": [
    "# check conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be13a49-2a7b-4272-8fb7-e81f684d8c31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecdb763-ee57-4137-aad3-265d3af4744b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph = networkx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9233d2e4-774c-4ebb-8eb0-f17733a9b1b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for img_1, img_2, sel_id in rank_pairs:\n",
    "for img_1, img_2, sel_id in rank_pairs.itertuples(index=False, name=None):\n",
    "    if sel_id == 0:\n",
    "        graph.add_edge(img_2, img_1)\n",
    "    else:\n",
    "        graph.add_edge(img_1, img_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff3d971-c385-4f26-b660-796dafa31fa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(graph.nodes), len(graph.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65815de0-5895-46b1-a45b-42468270df42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cycles = list(tqdm(networkx.simple_cycles(graph)))\n",
    "len(cycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f22964-9918-40d0-8a6f-1666d2b281cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subgraphs = list(networkx.weakly_connected_components(graph))\n",
    "len(subgraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c2d688-1612-488e-a590-0db4ce7a9e92",
   "metadata": {},
   "source": [
    "# check transitive relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f3cf9c-bbc6-491e-b4bc-75f0ef2c31ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trans_pairs = list()\n",
    "\n",
    "for image_2, d in networkx.all_pairs_shortest_path_length(graph):\n",
    "    \n",
    "    for image_1, dist in d.items():\n",
    "        \n",
    "        if dist <= 1:\n",
    "            continue\n",
    "        \n",
    "        trans_pairs.append((image_1, image_2, dist))\n",
    "        \n",
    "trans_pairs = pd.DataFrame(trans_pairs, columns=['image_1', 'image_2', 'dist'])\n",
    "trans_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509cbb2f-869c-4e86-8aeb-1e490b43c049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Environment",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
