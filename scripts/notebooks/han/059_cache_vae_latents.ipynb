{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "535d37a5-e217-4c6d-a0c1-896ee603db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import sys\n",
    "\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms.functional import to_tensor, to_pil_image, resize\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot\n",
    "from torchvision.transforms.functional import resize, center_crop\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f5853f9-19c3-4b3b-83ab-73d1bb47959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ebf189b-4953-4fee-a52b-ccb25e4ac337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3439a412486d4ed09e9b7dffd1e7262e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "from diffusers import AutoPipelineForText2Image\n",
    "\n",
    "model_id = 'runwayml/stable-diffusion-v1-5'\n",
    "\n",
    "stable_diffusion = AutoPipelineForText2Image.from_pretrained(\n",
    "    model_id, local_files_only=True, torch_dtype=torch.float16, variant=\"fp16\"\n",
    ")\n",
    "stable_diffusion.vae.eval().cuda();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b4816-5103-4f0a-9676-8b0e95a33719",
   "metadata": {},
   "source": [
    "# from jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1516fe6-9c9e-4268-a67b-3556b97f37d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(dataset_name):\n",
    "    \n",
    "    file_names = list()\n",
    "    for dname in os.listdir(os.path.join(ROOT, 'image', dataset_name)):\n",
    "        for fname in os.listdir(os.path.join(ROOT, 'image', dataset_name, dname)):\n",
    "            file_names.append(os.path.join(dname, fname))\n",
    "    \n",
    "    latents = list()\n",
    "    \n",
    "    for i in tqdm(range(0, len(file_names), BATCH_SIZE), leave=False):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "    \n",
    "            images = list()\n",
    "            for file_name in file_names[i:i+BATCH_SIZE]:\n",
    "                image = Image.open(os.path.join(ROOT, 'image', dataset_name, file_name)).convert('RGB')\n",
    "                images.append(to_tensor(image).half().cuda())\n",
    "            images = torch.stack(images, dim=0)\n",
    "        \n",
    "            images = (images - 0.5) * 2.0\n",
    "            latent = stable_diffusion.vae.encode(images).latent_dist.mean\n",
    "            \n",
    "            latents.append(latent.detach().cpu().numpy())\n",
    "    \n",
    "    latents = np.concatenate(latents, axis=0)\n",
    "    \n",
    "    np.savez(\n",
    "        os.path.join('./data', dataset_name, 'vae_latents.npz'),\n",
    "        latents=latents,\n",
    "        file_names=np.array(file_names)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd062799-c7e9-46f1-b571-f77e3469c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '../kcg-ml-image-pipeline/output/dataset/'\n",
    "\n",
    "DATASETs = [\n",
    "    'environmental',\n",
    "    'waifu',\n",
    "    'propaganda-poster'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e37147c9-ba06-4a7e-b12f-e11025d9dc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1156 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for dataset_name in DATASETs:\n",
    "    worker(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c74096-0c4b-4259-bc4b-7363818838a4",
   "metadata": {},
   "source": [
    "# from NPZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e5acb24-d9f7-403f-af1a-f62053fba28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(input_path, output_path):\n",
    "\n",
    "    f = zipfile.ZipFile(input_path)\n",
    "\n",
    "    file_names = list()\n",
    "    for fname in f.namelist():\n",
    "        if not fname.endswith('.jpg'):\n",
    "            continue\n",
    "        file_names.append(fname)\n",
    "\n",
    "    latents = list()\n",
    "    \n",
    "    for i in tqdm(range(0, len(file_names), BATCH_SIZE), leave=False):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "    \n",
    "            images = list()\n",
    "            for file_name in file_names[i:i+BATCH_SIZE]:\n",
    "                image = Image.open(f.open(file_name)).convert('RGB')\n",
    "                images.append(to_tensor(image).half().cuda())\n",
    "            images = torch.stack(images, dim=0)\n",
    "        \n",
    "            images = (images - 0.5) * 2.0\n",
    "            latent = stable_diffusion.vae.encode(images).latent_dist.mean\n",
    "            \n",
    "            latents.append(latent.detach().cpu().numpy())\n",
    "    \n",
    "    latents = np.concatenate(latents, axis=0)\n",
    "    \n",
    "    np.savez(\n",
    "        output_path,\n",
    "        latents=latents,\n",
    "        file_names=np.array(file_names)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2b3a005-2757-450a-b958-6530f5ffd8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    './generated/generated-1116.zip',\n",
    "    './generated/generated-1117.zip',\n",
    "    './generated/generated-1118.zip',\n",
    "    './generated/generated-1120.zip',\n",
    "    './generated/generated-1122.zip',\n",
    "    './generated/generated-1123.zip',\n",
    "    './generated/generated-1125.zip',\n",
    "    './generated/generated-1126.zip'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa31b42-4c38-4701-83ea-7d9bcb6d7e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7fc5ac1f7c420ca5f6f28c4c2062ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for file_path in file_paths:\n",
    "    worker(file_path, file_path.replace('.zip', '-vae_latents.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5cd5973-3ab1-41b7-951f-eea555f7b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    './generated/generated-1208-image.zip',\n",
    "    './generated/generated-1210-image.zip',\n",
    "    './generated/generated-1214-image.zip',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5219942c-60ff-4988-8d0f-b24c6ba115cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in file_paths:\n",
    "    worker(file_path, file_path.replace('-image.zip', '-vae_latents.npz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26562d3-d4c3-4531-8694-f995dd1dbf7b",
   "metadata": {},
   "source": [
    "# from jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c4252d0-db21-4d47-85ad-0b96a2c60d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(root, file_names, output_path):\n",
    "    \n",
    "    latents = list()\n",
    "    fnames = list()\n",
    "    for i in tqdm(range(0, len(file_names), BATCH_SIZE), leave=False):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "    \n",
    "            images = list()\n",
    "            for file_name in file_names[i:i+BATCH_SIZE]:\n",
    "                try:\n",
    "                    image = Image.open(os.path.join(root, file_name)).convert('RGB')#.resize((512, 512))\n",
    "                    image = center_crop(resize(image, size=512), output_size=(512, 512))\n",
    "                    images.append(to_tensor(image).half().cuda())\n",
    "                except:\n",
    "                    continue\n",
    "            images = torch.stack(images, dim=0)\n",
    "        \n",
    "            images = (images - 0.5) * 2.0\n",
    "            latent = stable_diffusion.vae.encode(images).latent_dist.mean\n",
    "            \n",
    "            latents.append(latent.detach().cpu().numpy())\n",
    "            fnames.append(file_name)\n",
    "    \n",
    "    latents = np.concatenate(latents, axis=0)\n",
    "    \n",
    "    np.savez(\n",
    "        output_path,\n",
    "        latents=latents,\n",
    "        file_names=np.array(fnames)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08836987-97a2-4013-84b5-c49767442301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ROOT = '../dataset/scrap/steam/'\n",
    "\n",
    "sizes = pd.read_csv(os.path.join(ROOT, 'sizes.csv'))\n",
    "\n",
    "file_names = sizes.query('width >= 512 and height >= 512 and (width / height) > 0.5 and (width / height) < 2.0 and aesthetic > 5. and nsfw < -2.')['file_name']\n",
    "file_names = list(file_names + '.jpg')\n",
    "\n",
    "# npz = np.load(os.path.join(ROOT, 'vae_latents.npz'))\n",
    "file_names = list(set(file_names).difference(npz['file_names']))\n",
    "\n",
    "worker(os.path.join(ROOT, 'screenshot'), file_names, os.path.join(ROOT, 'vae_latents-2.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8997acc9-2c5a-4ad5-a480-bf2179b300e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '../dataset/AVA/'\n",
    "\n",
    "sizes = pd.read_csv(os.path.join(ROOT, 'sizes.csv'))\n",
    "\n",
    "file_names = sizes.query('width >= 512 and height >= 512 and (width / height) > 0.9 and (width / height) < 1.1')['file_name']\n",
    "file_names = list(file_names)\n",
    "\n",
    "worker(os.path.join(ROOT, 'images'), file_names, os.path.join(ROOT, 'vae_latents.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d580e96-533e-49a0-ab5a-2527d0d1edc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ROOT = '../dataset/laion-art/'\n",
    "\n",
    "sizes = pd.read_csv('../dataset/laion-art/selected.csv')\n",
    "sizes = sizes.query('WIDTH >= 512 and HEIGHT >= 512 and (WIDTH / HEIGHT) > 0.9 and (WIDTH / HEIGHT) < 1.1')\n",
    "\n",
    "file_names = list()\n",
    "for index in sizes['index']:\n",
    "    file_name = f'{index}.jpeg'\n",
    "    if os.path.exists(os.path.join(ROOT, 'images', file_name)):\n",
    "        file_names.append(file_name)\n",
    "\n",
    "worker(os.path.join(ROOT, 'images'), file_names, os.path.join(ROOT, 'vae_latents.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee548766-ef37-4e8e-bae7-7d172c687260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kk",
   "language": "python",
   "name": "kk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
