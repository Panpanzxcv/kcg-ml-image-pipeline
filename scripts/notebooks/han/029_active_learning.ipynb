{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d6f8c9a-d2c7-473f-b909-c95c6ed6eb87",
   "metadata": {},
   "source": [
    "# Criteria\n",
    "\n",
    "## 1. Selected pairs of images with similar scores. \n",
    "\n",
    "    To get more detailed rank info. Because the scores are assigned by our scoring model, this might be accurate when predicting the selection of images with large score delta. But for images with small score delta, it performs badly.\n",
    "## 2. Selected pairs within similar images.\n",
    "\n",
    "    Currently,  annotatorâ€™s selection is strongly affected by the image topic or style. I.e., the annotator may alway choose Nintendo Mario style images, therefore, the scoring models will assign high scores to them. \n",
    "    This policy will force the annotator to focus on the image quality and may help us improve generation.\n",
    "\n",
    "# Implement\n",
    "\n",
    "## 1. Function: get_candidate_pairs_within_category\n",
    "\n",
    "I will first provide a general function to get candidate pairs within category\n",
    "\n",
    "Input:\n",
    "- categories: np.ndarray[int], shape is (N,)\n",
    "- max_pairs: int, max selecting pairs. \n",
    "- max_pairs should 0 < max_pairs < (N / n_categories) ** 2.\n",
    "    we will attempt to select (max_pairs / n_categories) pairs within each category.\n",
    "    \n",
    "Output:\n",
    "\n",
    "pairs: list[(index, index)], seleted pairs, index of input categories.\n",
    "\n",
    "\n",
    "## 2. Function: get_candidate_pairs_by_score\n",
    "\n",
    "I use 2 way to binning scores to categories:\n",
    "By fixed step bins\n",
    "By quantities\n",
    "\t\n",
    "\tI will provide a function to get candidate pairs with similar scores\n",
    "\n",
    "Input:\n",
    "- scores: np.ndarray[float], shape is (N,)\n",
    "- max_pairs: int, max selecting pairs. \n",
    "- n_bins: int, number of categories to be divided\n",
    "- use_quantities: bool, to use quantities or fixed step bins\n",
    "\n",
    "Output:\n",
    "\n",
    "pairs: list[(index, index)], seleted pairs, index of input scores.\n",
    "\t\n",
    "\n",
    "## 3. Function: get_candidate_pairs_by_embedding\n",
    "\t\n",
    "I use kmeans to divide images into categories of clusters.\n",
    "\n",
    "Input:\n",
    "- embeddings: np.ndarray, shape is (N, 768)\n",
    "- max_pairs: int, max selecting pairs. \n",
    "- n_clusters: int, number of categories to be divided\n",
    "\n",
    "Output:\n",
    "\n",
    "pairs: list[(index, index)], seleted pairs, index of input embeddings.\n",
    "\n",
    "These 2 criteria can be used with existing filters, we can filter images with score / variance / date, and pass the uuids and corresponding scores or embeddings to the function, and get candidate pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d3fa0b4-d4cb-44fa-b8d9-5bc3d0d26bca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import msgpack\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edc51109-4a36-4468-8c63-4062cc52dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../kcg-ml-image-pipeline/'))\n",
    "\n",
    "from utility.active_learning.pairs import get_candidate_pairs_by_score, get_candidate_pairs_by_embedding, embedding_to_category, get_candidate_pairs_within_category\n",
    "from utility.active_learning.samples import get_min_distance_to_representative_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4621601d-a939-4ab3-bbfa-bf29eddc439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '../dataset/'\n",
    "\n",
    "DATASETs = [\n",
    "    'environmental', \n",
    "    'character', \n",
    "    'icons', \n",
    "    'mech', \n",
    "    'waifu',\n",
    "    'propaganda-poster'\n",
    "]\n",
    "\n",
    "SAVE_DIR = './result/active_learning/1218/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640fd9da-7e0e-4cc1-a1cb-753a7854a0c2",
   "metadata": {},
   "source": [
    "# save image info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09b681da-76bb-42c6-bb0c-ace594ceb590",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = DATASETs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b3cd8f-b772-4651-ad05-f9e4c50c0e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_from_embs(embs, model, batch_size, preprocess=None):\n",
    "    \n",
    "    scores = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast(True):\n",
    "    \n",
    "            for i in tqdm(range(0, len(embs), batch_size), leave=False):\n",
    "                \n",
    "                x = torch.tensor(embs[i:i+batch_size]).cuda().half()\n",
    "                \n",
    "                if preprocess is not None:\n",
    "                    x = preprocess(x)\n",
    "    \n",
    "                score = model(x)[..., 0]\n",
    "    \n",
    "                scores.append(score.detach().cpu().numpy())\n",
    "\n",
    "    scores = np.concatenate(scores, axis=0)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a347f4e3-3e96-4cd5-a0ed-55e2310c05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image_info(dataset_name):\n",
    "\n",
    "    #\n",
    "    \n",
    "    js = json.load(open(f'data/{dataset_name}/data.json'))\n",
    "    \n",
    "    job_uuids = list()\n",
    "    samples = list()\n",
    "    \n",
    "    for info in tqdm(js.values(), total=len(js), leave=False):\n",
    "    \n",
    "        file_path = os.path.splitext(info['file_path'].split('_')[0])[0]\n",
    "        \n",
    "        path = os.path.join(ROOT, 'clip', f'{file_path}_clip.msgpack')\n",
    "    \n",
    "        with open(path, 'rb') as f:\n",
    "            mp = msgpack.load(f)\n",
    "    \n",
    "        job_uuids.append(info['job_uuid'])\n",
    "        samples.append(np.array(mp['clip-feature-vector']))\n",
    "    \n",
    "    job_uuids = np.array(job_uuids)\n",
    "    samples = np.concatenate(samples, axis=0)\n",
    "\n",
    "    #\n",
    "        \n",
    "    df = pd.DataFrame(\n",
    "        job_uuids.reshape(-1, 1), \n",
    "        columns=['job_uuid']\n",
    "    )\n",
    "    \n",
    "    # score\n",
    "    \n",
    "    vision_model = torch.nn.Linear(samples.shape[-1], 1, bias=True).cuda().eval()\n",
    "    vision_model.load_state_dict(torch.load(os.path.join('./weight/004', dataset_name, 'clip_vision.pt')))\n",
    "\n",
    "    npz = np.load(os.path.join('./weight/004', dataset_name, 'clip_vision.npz'))\n",
    "    mean, std = npz['mean'], npz['std']\n",
    "    \n",
    "    score = get_score_from_embs(samples, vision_model, batch_size=1024)\n",
    "    \n",
    "    df['sigma_score'] = (score - mean) / std\n",
    "    \n",
    "    # distance\n",
    "    \n",
    "    representative_names = json.load(open(os.path.join('./data', dataset_name, 'representative.json')))['representative']\n",
    "    representative_indices = list(map(path_to_index.get, representative_names))\n",
    "    representative_samples = samples[representative_indices]\n",
    "    \n",
    "    df['min_distance_to_representative_samples'] = get_min_distance_to_representative_samples(samples, representative_samples, distance_type='cosine')\n",
    "\n",
    "    # \n",
    "    \n",
    "    for n_clusters in [10, 100]:\n",
    "        if n_clusters > samples.shape[0] / 100:\n",
    "            break\n",
    "        df[f'category_{n_clusters}'] = embedding_to_category(embeddings=samples, n_clusters=n_clusters)\n",
    "\n",
    "    #\n",
    "\n",
    "    os.makedirs(os.path.join(SAVE_DIR, dataset_name), exist_ok=True)\n",
    "    \n",
    "    df.to_csv(os.path.join(SAVE_DIR, dataset_name, 'image_info.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52e1d939-7058-402a-9571-1be57498d3db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for dataset_name in DATASETs:\n",
    "    save_image_info(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377c9117-a896-4900-bc47-21c77723be86",
   "metadata": {},
   "source": [
    "# save_rank_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30a58a64-4bdc-4d0e-ae66-08037f90916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_uuid_pairs(df, pairs):\n",
    "    \n",
    "    indices_1, indices_2 = zip(*pairs)\n",
    "\n",
    "    job_uuid_1s = df['job_uuid'].iloc[list(indices_1)]\n",
    "    job_uuid_2s = df['job_uuid'].iloc[list(indices_2)]\n",
    "    \n",
    "    return [((job_uuid_1, job_uuid_2) if job_uuid_1 < job_uuid_2 else (job_uuid_2, job_uuid_1)) for job_uuid_1, job_uuid_2 in zip(job_uuid_1s, job_uuid_1s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96e5d603-9b31-4f55-b100-307842c375f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_rank_queue(dataset_name):\n",
    "\n",
    "    pmt_path = os.path.join('./data', dataset_name, 'prompt.json')\n",
    "    \n",
    "    prompts = json.load(open(pmt_path))\n",
    "    \n",
    "    ranked_pairs = set()\n",
    "    for fname in tqdm(os.listdir(os.path.join(ROOT, 'ranking', dataset_name)), leave=False):\n",
    "        js = json.load(open(os.path.join(ROOT, 'ranking', dataset_name, fname)))\n",
    "        \n",
    "        file_hash_1 = js['image_1_metadata']['file_hash']\n",
    "        file_hash_2 = js['image_2_metadata']['file_hash']\n",
    "    \n",
    "        try:\n",
    "            job_uuids_1 = prompts[file_hash_1]['job_uuid']\n",
    "            job_uuids_2 = prompts[file_hash_2]['job_uuid']\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        ranked_pairs.add((job_uuids_1, job_uuids_2))\n",
    "        ranked_pairs.add((job_uuids_2, job_uuids_1))\n",
    "    \n",
    "    #\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(SAVE_DIR, dataset_name, 'image_info.csv')).dropna()\n",
    "    df.query('sigma_score > .75', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #\n",
    "    \n",
    "    result = df.query(f'min_distance_to_representative_samples > 0.25')[['job_uuid']].copy()\n",
    "    result['policy'] = 'far_distance_to_ranked_images'\n",
    "    result.to_csv(os.path.join(SAVE_DIR, dataset_name, 'images.csv') , index=False)\n",
    "    \n",
    "    #\n",
    "    \n",
    "    results = list()\n",
    "    \n",
    "    for n_bins in [10, 100]:\n",
    "        \n",
    "        pairs = get_candidate_pairs_by_score(df['sigma_score'].values, max_pairs=1000, n_bins=n_bins, use_quantiles=True)\n",
    "        \n",
    "        pairs = get_job_uuid_pairs(df, pairs)\n",
    "        results.extend([pair + (f'same_sigma_score_bin_{n_bins}',) for pair in pairs if pair not in ranked_pairs])\n",
    "    \n",
    "    for n_clusters in [10, 100]:\n",
    "        \n",
    "        if f'category_{n_clusters}' not in df.columns:\n",
    "            break\n",
    "            \n",
    "        pairs = get_candidate_pairs_within_category(df[f'category_{n_clusters}'].values, max_pairs=1000)\n",
    "        \n",
    "        pairs = get_job_uuid_pairs(df, pairs)\n",
    "        results.extend([pair + (f'same_embedding_cluster_{n_bins}',) for pair in pairs if pair not in ranked_pairs])\n",
    "        \n",
    "    results = pd.DataFrame(results, columns=['job_uuid_1', 'job_uuid_2', 'policy'])\n",
    "    results.drop_duplicates(['job_uuid_1', 'job_uuid_2'], keep='first', inplace=True)\n",
    "    results.to_csv(os.path.join(SAVE_DIR, dataset_name, 'pairs.csv') , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f0da16f-8af6-4d32-bbce-ed1fcbff0543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/529 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for dataset_name in DATASETs:\n",
    "    save_rank_queue(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88310337-5f23-4cee-8886-57a15d0412e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# select images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "720fe0e8-1148-48a3-b34f-84834324e440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "09ae4c7a-ba22-470c-9610-ca3c467a4596",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(zip(npz['file_paths'], labels), columns=['file_path', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "56863736-1108-44c5-b3c7-bfddfb6bf1ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_dir = './image_clustering_clip_vision_kmeans'\n",
    "\n",
    "for c, g in df.groupby('label'):\n",
    "    \n",
    "    os.makedirs(os.path.join(target_dir, f'{c}'), exist_ok=True)\n",
    "    \n",
    "    if g.shape[0] < 5:\n",
    "        continue\n",
    "    \n",
    "    selected = np.random.choice(g['file_path'], 5, False)\n",
    "    \n",
    "    for file_path in selected:\n",
    "        file_path = os.path.join('../kcg-ml-image-pipeline/output/dataset/image/', file_path.split('_')[0] + '.jpg')\n",
    "        os.system(f'cp {file_path} {target_dir}/{c}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6071b0ba-e82d-4b0c-91e9-8d2211df33d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Environment",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
