{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdXwjD3TY1fO",
        "outputId": "821862f5-baaa-4b55-bd1d-303ee983915c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "TreeConnect_enhanced_BN(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv7): Conv2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  (bn7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (lc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), groups=2)\n",
            "  (bn_lc1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout1): Dropout(p=0.5, inplace=False)\n",
            "  (lc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), groups=4)\n",
            "  (bn_lc2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=4096, out_features=10, bias=True)\n",
            ")\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 44356002.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import transforms, datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "import time\n",
        "import numpy as np\n",
        "from torchsummary import summary\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.init as init\n",
        "import math\n",
        "\n",
        "\n",
        "architecture = 'treeConnect'\n",
        "\n",
        "\n",
        "\n",
        "########################################################## Global parameters ##########################################################\n",
        "\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "epochs = 5\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#optimizer = optim.Adam([{'params': model.parameters()}], lr=learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "#tree_connect_model = TreeConnect().to(device=device)\n",
        "#gModel = GeneralModel().to(device=device)\n",
        "# Display the model architecture\n",
        "#print(tree_connect_model)\n",
        "#print(gModel)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PyTorchModelTreeConnect(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PyTorchModelTreeConnect, self).__init__()\n",
        "        # Convolutional layers:\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)  # Assuming input channels = 3\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.conv6 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        # Reshaping:\n",
        "        #self.reshape = nn.Reshape((128, 128)) # done in forward\n",
        "\n",
        "\n",
        "        # # Locally connected layers (using nn.Conv1d for now):\n",
        "        # self.local_conv1 = nn.Conv1d(128, 16, kernel_size=1)  # Output 16 channels\n",
        "        # self.local_conv2 = nn.Conv1d(16, 16, kernel_size=1)  # Input 16 channels\n",
        "        # # Flattening:\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Dense layers:\n",
        "        self.dense1 = nn.Linear(2048, 256)  # Assuming output of reshape and local_conv2 is 4096\n",
        "        self.dense2 = nn.Linear(256, 128)\n",
        "        self.dense3 = nn.Linear(128, 10)  # Correct number of outputs\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the layers:\n",
        "        x = self.conv1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.conv4(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.conv5(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.conv6(x)\n",
        "        x = nn.functional.relu(x)\n",
        "\n",
        "\n",
        "        # x = x.view(-1, 128, 128)  # Reshape for local_conv1\n",
        "        # x = self.local_conv1(x)\n",
        "        # x = nn.functional.relu(x)\n",
        "\n",
        "        # x = x.permute(0, 2, 1)  # Simulate Permute((2, 1))\n",
        "        # x = x.reshape(-1, 16, 128)  # Reshape for local_conv2\n",
        "        # x = self.local_conv2(x)  # Now compatible with 16 input channels\n",
        "        # x = nn.functional.relu(x)\n",
        "\n",
        "        x = x.flatten(start_dim=1)   # Reshape to (64, 4096)\n",
        "        x = self.dense1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.dense2(x)\n",
        "        x = nn.functional.relu(x)\n",
        "\n",
        "        # Standard\n",
        "        #x = x.view(x.size(0), -1)\n",
        "        x = self.dense3(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "    #     # Convolutional layers:\n",
        "    #     self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
        "    #     self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "    #     self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
        "    #     self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "    #     self.conv5 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "    #     self.conv6 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    #     # Locally connected layers:\n",
        "    #     self.local_conv1 = nn.Conv1d(128, 16, kernel_size=1)\n",
        "    #     self.local_conv2 = nn.Conv1d(16, 16, kernel_size=1)\n",
        "\n",
        "    #     # Flattening:\n",
        "    #     self.flatten = nn.Flatten()\n",
        "\n",
        "    #     # Dense layers:\n",
        "    #     self.dense1 = nn.Linear(128 * 128, 256)\n",
        "    #     self.dense2 = nn.Linear(256, 128)\n",
        "    #     self.dense3 = nn.Linear(128, 10)\n",
        "\n",
        "    # def forward(self, x):\n",
        "    #   x = F.relu(self.conv1(x))\n",
        "    #   x = F.relu(self.conv2(x))\n",
        "    #   x = F.relu(self.conv3(x))\n",
        "    #   x = F.relu(self.conv4(x))\n",
        "    #   x = F.relu(self.conv5(x))\n",
        "    #   x = F.relu(self.conv6(x))\n",
        "\n",
        "    #   x = x.view(-1, 128, 128)\n",
        "    #   x = F.relu(self.local_conv1(x))\n",
        "    #   x = x.permute(0, 2, 1)\n",
        "    #   x = x.reshape(-1, 16, 128)  # Updated line to use reshape\n",
        "    #   x = F.relu(self.local_conv2(x))\n",
        "\n",
        "    #   x = x.reshape(-1, 128 * 128)  # Reshape for the dense layer\n",
        "    #   x = F.relu(self.dense1(x))\n",
        "    #   x = F.relu(self.dense2(x))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# General Class\n",
        "class GeneralModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GeneralModel, self).__init__()\n",
        "        # Standard Conv Block\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv6 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv7 = nn.Conv2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "\n",
        "        if architecture == 'treeConnect':\n",
        "          # Locally conntected layers\n",
        "          self.lc1 = nn.Conv2d(128, 64, kernel_size=1, groups=2)  # Adjusted for 4 groups with 4 channels each\n",
        "          self.lc2 = nn.Conv2d(64, 256, kernel_size=1, groups=4)  # Adjusted for 4 groups with 64 channels each\n",
        "          self.fc = nn.Linear(64 * 64, 10)  # Last layer\n",
        "\n",
        "        else: # it's FC\n",
        "           self.fc1 = nn.Linear(2048, 256) # FC\n",
        "           self.fc2 = nn.Linear(256, 256) # FC\n",
        "           self.fc = nn.Linear(256, 10)  # Last layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = F.relu(self.conv6(x))\n",
        "        x = F.relu(self.conv7(x))\n",
        "\n",
        "        if architecture == 'treeConnect':\n",
        "          x = F.relu(self.lc1(x))\n",
        "          x = F.relu(self.lc2(x))\n",
        "        else:\n",
        "          # Flatter the output\n",
        "          x = x.view(x.size(0), -1)\n",
        "          x = F.relu(self.fc1(x))\n",
        "          x = F.relu(self.fc2(x))\n",
        "\n",
        "        # Standard\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TreeConnect(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TreeConnect, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv6 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv7 = nn.Conv2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        # Locally connected part\n",
        "        # self.lc1 = nn.Conv2d(128, 16128, kernel_size=1, groups=128)\n",
        "        # self.lc2 = nn.Conv2d(16128, 256, kernel_size=1, groups=16)\n",
        "        # Locally connected layers\n",
        "        self.lc1 = nn.Conv2d(128, 64, kernel_size=1, groups=2)  # Adjusted for 4 groups with 4 channels each\n",
        "        self.lc2 = nn.Conv2d(64, 256, kernel_size=1, groups=4)  # Adjusted for 4 groups with 64 channels each\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Fully connected layers\n",
        "        #self.fc = nn.Linear(64 * 64, 10)\n",
        "        #self.fc1 = nn.Linear(16 * 128, 256)\n",
        "        self.fc = nn.Linear(64 * 64, 10)  # Assuming 10 classes for CIFAR-10\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = F.relu(self.conv6(x))\n",
        "        x = F.relu(self.conv7(x))\n",
        "\n",
        "        #x = x.view(x.size(0), 128, -1)\n",
        "        x = F.relu(self.lc1(x))\n",
        "        #x = x.view(x.size(0), 16, -1).permute(0, 2, 1)\n",
        "        x = F.relu(self.lc2(x))\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TreeConnect_enhanced(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TreeConnect_enhanced, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv6 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv7 = nn.Conv2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        # Locally connected part\n",
        "        # self.lc1 = nn.Conv2d(128, 16128, kernel_size=1, groups=128)\n",
        "        # self.lc2 = nn.Conv2d(16128, 256, kernel_size=1, groups=16)\n",
        "        # Locally connected layers\n",
        "        self.lc1 = nn.Conv2d(128, 64, kernel_size=1, groups=2)  # Adjusted for 4 groups with 4 channels each\n",
        "        self.dropout1 = nn.Dropout(0.6)  # Add dropout with a probability of 0.5\n",
        "        self.lc2 = nn.Conv2d(64, 256, kernel_size=1, groups=4)  # Adjusted for 4 groups with 64 channels each\n",
        "        self.dropout2 = nn.Dropout(0.6)  # Add dropout with a probability of 0.5\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Fully connected layers\n",
        "        #self.fc = nn.Linear(64 * 64, 10)\n",
        "        #self.fc1 = nn.Linear(16 * 128, 256)\n",
        "        self.fc = nn.Linear(64 * 64, 10)  # Assuming 10 classes for CIFAR-10\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = F.relu(self.conv6(x))\n",
        "        x = F.relu(self.conv7(x))\n",
        "\n",
        "        #x = x.view(x.size(0), 128, -1)\n",
        "        x = F.relu(self.lc1(x))\n",
        "        self.dropout1 = nn.Dropout(0.6)  # Add dropout with a probability of 0.5\n",
        "        #x = x.view(x.size(0), 16, -1).permute(0, 2, 1)\n",
        "        x = F.relu(self.lc2(x))\n",
        "        self.dropout2 = nn.Dropout(0.6)  # Add dropout with a probability of 0.5\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TreeConnect_enhanced_BN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TreeConnect_enhanced_BN, self).__init__()\n",
        "        # Convolutional layers with BatchNorm\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(128)\n",
        "        self.conv6 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(256)\n",
        "        self.conv7 = nn.Conv2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.bn7 = nn.BatchNorm2d(128)\n",
        "\n",
        "        # Locally connected layers with BatchNorm and Dropout\n",
        "        self.lc1 = nn.Conv2d(128, 64, kernel_size=1, groups=2)  # Adjusted for 4 groups with 4 channels each\n",
        "        self.bn_lc1 = nn.BatchNorm2d(64)\n",
        "        self.dropout1 = nn.Dropout(0.5)  # Changed probability to 0.5 for consistency\n",
        "        self.lc2 = nn.Conv2d(64, 256, kernel_size=1, groups=4)  # Adjusted for 4 groups with 64 channels each\n",
        "        self.bn_lc2 = nn.BatchNorm2d(256)\n",
        "        self.dropout2 = nn.Dropout(0.5)  # Changed probability to 0.5 for consistency\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(64 * 64, 10)  # Assuming 10 classes for CIFAR-10\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.bn4(x)\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = self.bn5(x)\n",
        "        x = F.relu(self.conv6(x))\n",
        "        x = self.bn6(x)\n",
        "        x = F.relu(self.conv7(x))\n",
        "        x = self.bn7(x)\n",
        "\n",
        "        x = F.relu(self.lc1(x))\n",
        "        x = self.bn_lc1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.lc2(x))\n",
        "        x = self.bn_lc2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#######################\n",
        "\n",
        "\n",
        "class FullConnect(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FullConnect, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv6 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv7 = nn.Conv2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(2048, 256)  # Updated to match the number of features\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc = nn.Linear(256, 10)  # Assuming 10 classes for CIFAR-10\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = F.relu(self.conv6(x))\n",
        "        x = F.relu(self.conv7(x))\n",
        "\n",
        "        # Flatten the output\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        #x = x.view(x.size(0), 16, -1).permute(0, 2, 1)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class FullConnect_fat(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FullConnect_fat, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv6 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv7 = nn.Conv2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(2048, 2048)  # Updated to match the number of features\n",
        "        self.fc2 = nn.Linear(2048, 2048)\n",
        "        self.fc3 = nn.Linear(2048, 2048)\n",
        "        self.fc4 = nn.Linear(2048, 2048)\n",
        "        self.fc5 = nn.Linear(2048, 2048)\n",
        "        self.fc6 = nn.Linear(2048, 2048)\n",
        "        self.fc7 = nn.Linear(2048, 256)\n",
        "        self.fc8 = nn.Linear(256, 256)\n",
        "        self.fc9 = nn.Linear(256, 256)\n",
        "        self.fc10 = nn.Linear(256, 256)\n",
        "        self.fc11 = nn.Linear(256, 256)\n",
        "        self.fc12 = nn.Linear(256, 256)\n",
        "        self.fc13 = nn.Linear(256, 256)\n",
        "        self.fc = nn.Linear(256, 10)  # Assuming 10 classes for CIFAR-10\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = F.relu(self.conv6(x))\n",
        "        x = F.relu(self.conv7(x))\n",
        "\n",
        "        # Flatten the output\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        #x = x.view(x.size(0), 16, -1).permute(0, 2, 1)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = F.relu(self.fc5(x))\n",
        "        x = F.relu(self.fc6(x))\n",
        "        x = F.relu(self.fc7(x))\n",
        "        x = F.relu(self.fc8(x))\n",
        "        x = F.relu(self.fc9(x))\n",
        "        x = F.relu(self.fc10(x))\n",
        "        x = F.relu(self.fc11(x))\n",
        "        x = F.relu(self.fc12(x))\n",
        "        x = F.relu(self.fc13(x))\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "class FullConnect_enhanced(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FullConnect_enhanced, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(128)\n",
        "        self.conv6 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(256)\n",
        "        self.conv7 = nn.Conv2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.bn7 = nn.BatchNorm2d(128)\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(2048, 256)  # Updated to match the number of features\n",
        "        self.dropout1 = nn.Dropout(0.5)  # Add dropout with a probability of 0.5\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.dropout2 = nn.Dropout(0.5)  # Add dropout with a probability of 0.5\n",
        "        self.fc = nn.Linear(256, 10)  # Assuming 10 classes for CIFAR-10\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.bn4(x)\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = self.bn5(x)\n",
        "        x = F.relu(self.conv6(x))\n",
        "        x = self.bn6(x)\n",
        "        x = F.relu(self.conv7(x))\n",
        "        x = self.bn7(x)\n",
        "\n",
        "        # Flatten the output\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x) # Dropout 1\n",
        "        #x = x.view(x.size(0), 16, -1).permute(0, 2, 1)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x) # Dropout 2\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "class RegularSparseLocalLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, distance_factor):\n",
        "        super(RegularSparseLocalLayer, self).__init__()\n",
        "\n",
        "        self.mask = nn.Parameter(torch.zeros(out_features, in_features), requires_grad=False)\n",
        "        self.distance_factor = distance_factor\n",
        "\n",
        "\n",
        "        # Set a regular sparsity pattern based on distance\n",
        "        for i in range(out_features):\n",
        "            start = max(0, i - distance_factor)  # Ensure start index is within bounds\n",
        "            end = min(in_features, i + distance_factor + 1)  # Ensure end index is within bounds\n",
        "            self.mask[i, start:end] = 1\n",
        "\n",
        "\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "\n",
        "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
        "        fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def forward(self, x):\n",
        "        masked_weight = self.mask * self.weight\n",
        "        return F.linear(x, masked_weight, self.bias)\n",
        "\n",
        "class RegularSparseLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, sparsity_factor):\n",
        "        super(RegularSparseLayer, self).__init__()\n",
        "\n",
        "        self.mask = nn.Parameter(torch.zeros(out_features, in_features), requires_grad=False)\n",
        "        self.sparsity_factor = sparsity_factor\n",
        "\n",
        "        # Set a regular sparsity pattern where each neuron is connected to a fixed number of neurons\n",
        "        for i in range(out_features):\n",
        "            self.mask[i, i * sparsity_factor : (i + 1) * sparsity_factor] = 1\n",
        "\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "\n",
        "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
        "        fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def forward(self, x):\n",
        "        masked_weight = self.mask * self.weight\n",
        "        return F.linear(x, masked_weight, self.bias)\n",
        "\n",
        "class FractionalSparseLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, connection_fraction):\n",
        "        super(FractionalSparseLayer, self).__init__()\n",
        "\n",
        "        self.mask = nn.Parameter(torch.zeros(out_features, in_features), requires_grad=False)\n",
        "        self.connection_fraction = connection_fraction\n",
        "\n",
        "        # Calculate the number of connections to keep for each neuron\n",
        "        num_connections_to_keep = int(in_features * connection_fraction)\n",
        "\n",
        "        # Set a random sparsity pattern for each neuron\n",
        "        for i in range(out_features):\n",
        "            indices_to_keep = torch.randperm(in_features)[:num_connections_to_keep]\n",
        "            self.mask[i, indices_to_keep] = 1\n",
        "\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "\n",
        "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
        "        fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def forward(self, x):\n",
        "        masked_weight = self.mask * self.weight\n",
        "        return F.linear(x, masked_weight, self.bias)\n",
        "\n",
        "\n",
        "\n",
        "class RandomSparseLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, percentage_masked):\n",
        "        super(RandomSparseLayer, self).__init__()\n",
        "\n",
        "        self.mask = nn.Parameter(torch.ones(out_features, in_features), requires_grad=False)\n",
        "        self.prune_percentage = percentage_masked\n",
        "\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "\n",
        "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
        "        fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "        self.reset_mask()\n",
        "\n",
        "    def reset_mask(self):\n",
        "        num_elements = int(self.mask.numel() * (self.prune_percentage / 100.0))\n",
        "        indices_to_prune = torch.randperm(self.mask.numel())[:num_elements]\n",
        "        self.mask.view(-1)[indices_to_prune] = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        masked_weight = self.mask * self.weight\n",
        "        return F.linear(x, masked_weight, self.bias)\n",
        "\n",
        "\n",
        "\n",
        "class Random_Sparse_enhanced(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Random_Sparse_enhanced, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(128)\n",
        "        self.conv6 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(256)\n",
        "        self.conv7 = nn.Conv2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.bn7 = nn.BatchNorm2d(128)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # # Fractional Sparse Layers\n",
        "        # connection_fraction1 = 0.5  # Connect each neuron to half of the original connections\n",
        "        # self.sparse_layer1 = FractionalSparseLayer(2048, 256, connection_fraction1)\n",
        "\n",
        "        # Random Sparse Layers RegularSparseLocalLayer\n",
        "        self.sparse_layer1 = RegularSparseLocalLayer(2048, 256, distance_factor=64)\n",
        "        #self.dropout1 = nn.Dropout(0.6)\n",
        "        self.sparse_layer2 = RegularSparseLocalLayer(256, 256, distance_factor=32)\n",
        "        #self.dropout2 = nn.Dropout(0.6)\n",
        "\n",
        "\n",
        "        self.fc = nn.Linear(256, 10)  # Assuming 10 classes for CIFAR-10\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.bn4(x)\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = self.bn5(x)\n",
        "        x = F.relu(self.conv6(x))\n",
        "        x = self.bn6(x)\n",
        "        x = F.relu(self.conv7(x))\n",
        "        x = self.bn7(x)\n",
        "\n",
        "        # Flatten the output\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = F.relu(self.sparse_layer1(x))\n",
        "        #x = self.dropout1(x)  # Dropout 1\n",
        "        x = F.relu(self.sparse_layer2(x))\n",
        "        #x = self.dropout2(x)  # Dropout 2\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "# class GlobalAveragePooling_enhanced(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(GlobalAveragePooling_enhanced, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "#         self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "#         self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
        "#         self.conv4 = nn.Conv2d(128, 128, kernel_size=4, stride=2, padding=1)\n",
        "#         self.conv5 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "#         self.conv6 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "#         self.conv7 = nn.Conv2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "#         # Replace Fully Connected layers with:\n",
        "#         self.gap = nn.AvgPool2d(kernel_size=self.conv7.kernel_size)  # Global Average Pooling\n",
        "#         self.embedding = nn.Embedding(128, 10)  # Embedding layer with output for 10 classes\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = F.relu(self.conv1(x))\n",
        "#         x = F.relu(self.conv2(x))\n",
        "#         x = F.relu(self.conv3(x))\n",
        "#         x = F.relu(self.conv4(x))\n",
        "#         x = F.relu(self.conv5(x))\n",
        "#         x = F.relu(self.conv6(x))\n",
        "#         x = F.relu(self.conv7(x))\n",
        "#         # Replace flattening and FC layers with:\n",
        "#         x = self.gap(x)  # Apply Global Average Pooling\n",
        "#         x = x.view(-1, 128)  # Reshape to batch_size x embedding_dim\n",
        "#         x = self.embedding(x)  # Embed features based on class probability\n",
        "\n",
        "#         # Further processing or classification with embedded vectors ...\n",
        "\n",
        "#         return x  # Replace log_softmax with your specific processing\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from torch.cuda.amp import autocast\n",
        "from torch.cuda.amp import GradScaler\n",
        "from torch.nn import MSELoss, CrossEntropyLoss, L1Loss\n",
        "\n",
        "def TrainModel_Automated_Mixed_Precision(modelclass,modelSaveName):\n",
        "\n",
        "\n",
        "  # Create model\n",
        "  model = modelclass().to(device=device)\n",
        "  print(model)\n",
        "\n",
        "\n",
        "  # Define data transformations\n",
        "  transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "  # Download CIFAR-10 training dataset\n",
        "  train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "  # Create a DataLoader for the training dataset\n",
        "  train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "  # Training parameters\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  # weight_decay=1e-4\n",
        "  #optimizer = optim.Adam([{'params': model.parameters()}], lr=learning_rate,weight_decay=0.0001)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.01 )\n",
        "  scaler = GradScaler()\n",
        "# Lists to store training and validation metrics\n",
        "  train_loss_list = []\n",
        "  train_accuracy_list = []\n",
        "  val_loss_list = []\n",
        "  val_accuracy_list = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Record the start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  # Training loop\n",
        "  for epoch in range(epochs):\n",
        "      model.train()\n",
        "      total_loss = 0.0\n",
        "      correct_train = 0\n",
        "      total_train = 0\n",
        "\n",
        "      for inputs, labels in train_loader:\n",
        "          optimizer.zero_grad()\n",
        "          with autocast(dtype=torch.float16):\n",
        "            output = model(input)\n",
        "            loss = criterion(outputs.to(device), labels.to(device))\n",
        "\n",
        "\n",
        "          scaler.scale(loss).backward()\n",
        "          scaler.step(optimizer)\n",
        "          scaler.update()\n",
        "\n",
        "          total_loss += loss.item()\n",
        "          _, predicted = outputs.max(1)\n",
        "          total_train += labels.size(0)\n",
        "          correct_train += predicted.eq(labels.to(device)).sum().item()\n",
        "\n",
        "      # Calculate training accuracy and loss\n",
        "      train_accuracy = 100.0 * correct_train / total_train\n",
        "      train_loss = total_loss / len(train_loader)\n",
        "\n",
        "      # Validation\n",
        "      model.eval()\n",
        "      total_val_loss = 0.0\n",
        "      correct_val = 0\n",
        "      total_val = 0\n",
        "\n",
        "\n",
        "      # Load the validation set\n",
        "      val_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "      # Create a DataLoader for the validation dataset\n",
        "      batch_size_val = 64\n",
        "      val_loader = DataLoader(val_dataset, batch_size=batch_size_val, shuffle=False)\n",
        "\n",
        "      # Run validation tests\n",
        "      with torch.no_grad():\n",
        "          for inputs, labels in val_loader:\n",
        "              outputs = model(inputs.to(device=device))\n",
        "              loss = criterion(outputs.to(device=device), labels.to(device=device))\n",
        "\n",
        "              total_val_loss += loss.item()\n",
        "              _, predicted = outputs.max(1)\n",
        "              total_val += labels.size(0)\n",
        "              correct_val += predicted.eq(labels.to(device=device)).sum().item()\n",
        "\n",
        "      # Calculate validation accuracy and loss\n",
        "      val_accuracy = 100.0 * correct_val / total_val\n",
        "      val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "      # Append metrics to lists\n",
        "      train_loss_list.append(train_loss)\n",
        "      train_accuracy_list.append(train_accuracy)\n",
        "      val_loss_list.append(val_loss)\n",
        "      val_accuracy_list.append(val_accuracy)\n",
        "\n",
        "      # Print and display progress\n",
        "      print(f'Epoch [{epoch+1}/{epochs}], '\n",
        "            f'Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%, '\n",
        "            f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "\n",
        "\n",
        "  # Record the end time\n",
        "  end_time = time.time()\n",
        "\n",
        "  # Calculate the elapsed time\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(\"Time elapsed: \",elapsed_time)\n",
        "\n",
        "  # Plotting the training and validation curves\n",
        "  plt.figure(figsize=(12, 4))\n",
        "\n",
        "  # Plotting the loss curves\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(train_loss_list, label='Training Loss', marker='o')\n",
        "  plt.plot(val_loss_list, label='Validation Loss', marker='o')\n",
        "  plt.title('Loss Curves')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  # Plotting the accuracy curves\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(train_accuracy_list, label='Training Accuracy', marker='o')\n",
        "  plt.plot(val_accuracy_list, label='Validation Accuracy', marker='o')\n",
        "  plt.title('Accuracy Curves')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy (%)')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  # Save the trained model\n",
        "  torch.save(model.state_dict(), modelSaveName)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################################################\n",
        "# Train and plot (AMP V2)\n",
        "########################################################################\n",
        "\n",
        "\n",
        "def TrainModel_AMP_V2(modelclass,modelSaveName):\n",
        "\n",
        "\n",
        "  # Create model\n",
        "  model = modelclass().to(device=device)\n",
        "  print(model)\n",
        "\n",
        "\n",
        "  # Define data transformations\n",
        "  transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "  # Download CIFAR-10 training dataset\n",
        "  train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "  # Create a DataLoader for the training dataset\n",
        "  train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "# Lists to store training and validation metrics\n",
        "  train_loss_list = []\n",
        "  train_accuracy_list = []\n",
        "  val_loss_list = []\n",
        "  val_accuracy_list = []\n",
        "\n",
        "  # Training parameters\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam([{'params': model.parameters()}], lr=learning_rate, weight_decay=0.0001)\n",
        "\n",
        "  scaler = GradScaler()\n",
        "\n",
        "  # Record the start time\n",
        "  start_time = time.time()\n",
        "\n",
        "\n",
        "  # Training loop\n",
        "  for epoch in range(epochs):\n",
        "      model.train()\n",
        "      total_loss = 0.0\n",
        "      correct_train = 0\n",
        "      total_train = 0\n",
        "\n",
        "      for inputs, labels in train_loader:\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          with autocast():\n",
        "              outputs = model(inputs.to(device))\n",
        "              loss = criterion(outputs.to(device), labels.to(device))\n",
        "\n",
        "          scaler.scale(loss).backward()\n",
        "          scaler.step(optimizer)\n",
        "          scaler.update()\n",
        "\n",
        "          total_loss += loss.item()\n",
        "          _, predicted = outputs.max(1)\n",
        "          total_train += labels.size(0)\n",
        "          correct_train += predicted.eq(labels.to(device)).sum().item()\n",
        "\n",
        "\n",
        "      # Calculate training accuracy and loss\n",
        "      train_accuracy = 100.0 * correct_train / total_train\n",
        "      train_loss = total_loss / len(train_loader)\n",
        "\n",
        "      # Validation\n",
        "      model.eval()\n",
        "      total_val_loss = 0.0\n",
        "      correct_val = 0\n",
        "      total_val = 0\n",
        "\n",
        "\n",
        "      # Load the validation set\n",
        "      val_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "      # Create a DataLoader for the validation dataset\n",
        "      batch_size_val = 64\n",
        "      val_loader = DataLoader(val_dataset, batch_size=batch_size_val, shuffle=False)\n",
        "\n",
        "      # Run validation tests\n",
        "      with torch.no_grad():\n",
        "          for inputs, labels in val_loader:\n",
        "              outputs = model(inputs.to(device=device))\n",
        "              loss = criterion(outputs.to(device=device), labels.to(device=device))\n",
        "\n",
        "              total_val_loss += loss.item()\n",
        "              _, predicted = outputs.max(1)\n",
        "              total_val += labels.size(0)\n",
        "              correct_val += predicted.eq(labels.to(device=device)).sum().item()\n",
        "\n",
        "      # Calculate validation accuracy and loss\n",
        "      val_accuracy = 100.0 * correct_val / total_val\n",
        "      val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "      # Append metrics to lists\n",
        "      train_loss_list.append(train_loss)\n",
        "      train_accuracy_list.append(train_accuracy)\n",
        "      val_loss_list.append(val_loss)\n",
        "      val_accuracy_list.append(val_accuracy)\n",
        "\n",
        "      # Print and display progress\n",
        "      print(f'Epoch [{epoch+1}/{epochs}], '\n",
        "            f'Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%, '\n",
        "            f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "\n",
        "\n",
        "  # Record the end time\n",
        "  end_time = time.time()\n",
        "\n",
        "  # Calculate the elapsed time\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(\"Time elapsed: \",elapsed_time)\n",
        "\n",
        "  # Plotting the training and validation curves\n",
        "  plt.figure(figsize=(12, 4))\n",
        "\n",
        "  # Plotting the loss curves\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(train_loss_list, label='Training Loss', marker='o')\n",
        "  plt.plot(val_loss_list, label='Validation Loss', marker='o')\n",
        "  plt.title('Loss Curves')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  # Plotting the accuracy curves\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(train_accuracy_list, label='Training Accuracy', marker='o')\n",
        "  plt.plot(val_accuracy_list, label='Validation Accuracy', marker='o')\n",
        "  plt.title('Accuracy Curves')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy (%)')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  # Save the trained model\n",
        "  torch.save(model.state_dict(), modelSaveName)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################################################\n",
        "# Train and plot (Standard)\n",
        "########################################################################\n",
        "\n",
        "def TrainModel(modelclass,modelSaveName):\n",
        "\n",
        "\n",
        "  # Create model\n",
        "  model = modelclass().to(device=device)\n",
        "  print(model)\n",
        "\n",
        "\n",
        "  # Define data transformations\n",
        "  transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "  # Download CIFAR-10 training dataset\n",
        "  train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "  # Create a DataLoader for the training dataset\n",
        "  train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "  # Training parameters\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  # weight_decay=1e-4\n",
        "  optimizer = optim.Adam([{'params': model.parameters()}], lr=learning_rate,weight_decay=0.0001)\n",
        "\n",
        "\n",
        "# Lists to store training and validation metrics\n",
        "  train_loss_list = []\n",
        "  train_accuracy_list = []\n",
        "  val_loss_list = []\n",
        "  val_accuracy_list = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Record the start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  # Training loop\n",
        "  for epoch in range(epochs):\n",
        "      model.train()\n",
        "      total_loss = 0.0\n",
        "      correct_train = 0\n",
        "      total_train = 0\n",
        "\n",
        "      for inputs, labels in train_loader:\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(inputs.to(device))\n",
        "          loss = criterion(outputs.to(device), labels.to(device))\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          total_loss += loss.item()\n",
        "          _, predicted = outputs.max(1)\n",
        "          total_train += labels.size(0)\n",
        "          correct_train += predicted.eq(labels.to(device)).sum().item()\n",
        "\n",
        "      # Calculate training accuracy and loss\n",
        "      train_accuracy = 100.0 * correct_train / total_train\n",
        "      train_loss = total_loss / len(train_loader)\n",
        "\n",
        "      # Validation\n",
        "      model.eval()\n",
        "      total_val_loss = 0.0\n",
        "      correct_val = 0\n",
        "      total_val = 0\n",
        "\n",
        "\n",
        "      # Load the validation set\n",
        "      val_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "      # Create a DataLoader for the validation dataset\n",
        "      batch_size_val = 64\n",
        "      val_loader = DataLoader(val_dataset, batch_size=batch_size_val, shuffle=False)\n",
        "\n",
        "      # Run validation tests\n",
        "      with torch.no_grad():\n",
        "          for inputs, labels in val_loader:\n",
        "              outputs = model(inputs.to(device=device))\n",
        "              loss = criterion(outputs.to(device=device), labels.to(device=device))\n",
        "\n",
        "              total_val_loss += loss.item()\n",
        "              _, predicted = outputs.max(1)\n",
        "              total_val += labels.size(0)\n",
        "              correct_val += predicted.eq(labels.to(device=device)).sum().item()\n",
        "\n",
        "      # Calculate validation accuracy and loss\n",
        "      val_accuracy = 100.0 * correct_val / total_val\n",
        "      val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "      # Append metrics to lists\n",
        "      train_loss_list.append(train_loss)\n",
        "      train_accuracy_list.append(train_accuracy)\n",
        "      val_loss_list.append(val_loss)\n",
        "      val_accuracy_list.append(val_accuracy)\n",
        "\n",
        "      # Print and display progress\n",
        "      print(f'Epoch [{epoch+1}/{epochs}], '\n",
        "            f'Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%, '\n",
        "            f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "\n",
        "\n",
        "  # Record the end time\n",
        "  end_time = time.time()\n",
        "\n",
        "  # Calculate the elapsed time\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(\"Time elapsed: \",elapsed_time)\n",
        "\n",
        "  # Plotting the training and validation curves\n",
        "  plt.figure(figsize=(12, 4))\n",
        "\n",
        "  # Plotting the loss curves\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(train_loss_list, label='Training Loss', marker='o')\n",
        "  plt.plot(val_loss_list, label='Validation Loss', marker='o')\n",
        "  plt.title('Loss Curves')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  # Plotting the accuracy curves\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(train_accuracy_list, label='Training Accuracy', marker='o')\n",
        "  plt.plot(val_accuracy_list, label='Validation Accuracy', marker='o')\n",
        "  plt.title('Accuracy Curves')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy (%)')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  # Save the trained model\n",
        "  torch.save(model.state_dict(), modelSaveName)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "######################################################################################################\n",
        "#                                Run inference -> Evaluation\n",
        "######################################################################################################\n",
        "\n",
        "def evalualte_model(modelclass,modelName,batchSize):\n",
        "  # Define the transformation for your images\n",
        "  transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "  # Validation dataset\n",
        "  val_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "  # Create a DataLoader for the validation dataset\n",
        "  batch_size = batchSize  # BS\n",
        "  test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  total_val_loss = 0.0\n",
        "  correct_val = 0\n",
        "  total_val = 0\n",
        "\n",
        "  # load model\n",
        "  model =  modelclass().to(device) # FullConnect # TreeConnect()\n",
        "  # Load the pre-trained weights\n",
        "  model.load_state_dict(torch.load(modelName))\n",
        "\n",
        "  #Run validation tests\n",
        "  with torch.no_grad():\n",
        "      for inputs, labels in test_loader:\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          total_val_loss += loss.item()\n",
        "          _, predicted = outputs.max(1)\n",
        "          total_val += labels.size(0)\n",
        "          correct_val += predicted.eq(labels).sum().item()\n",
        "\n",
        "  # Calculate accuracy and average loss\n",
        "  test_accuracy = 100.0 * correct_val / total_val\n",
        "  val_loss = total_val_loss / len(test_loader)\n",
        "\n",
        "  # print Validation Accuracy\n",
        "  print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "\n",
        "######################################################################################################\n",
        "#                                Run inference -> Speed\n",
        "######################################################################################################\n",
        "\n",
        "def calculate_model_speed(modelclass,modelName,batchsize):\n",
        "\n",
        "  # Define the transformation for your images\n",
        "  transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "  # Create a DataLoader for the validation dataset\n",
        "  batch_size = batchsize  # BS\n",
        "  # Test data 10000\n",
        "  test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  # Check images number\n",
        "  total_images = len(test_loader.dataset)\n",
        "  print(f'Total number of images in the test set: {total_images}')\n",
        "\n",
        "  # load model\n",
        "  model = modelclass().to(device=device) # FullConnect() #  TreeConnect()\n",
        "  # Load the pre-trained weights\n",
        "  model.load_state_dict(torch.load(modelName))\n",
        "  # Start inference time\n",
        "  inf_start_time = time.time()\n",
        "  #Evaluate the model\n",
        "  model.eval()\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Run inference on the test dataset\n",
        "  # Lists to store predictions and ground truth labels\n",
        "  all_predictions = []\n",
        "  all_labels = []\n",
        "  with torch.no_grad():\n",
        "      for inputs , labels in test_loader:\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "          outputs = model(inputs)\n",
        "          _, predicted = outputs.max(1)\n",
        "\n",
        "  # End inference time\n",
        "  inf_end_time = time.time()\n",
        "\n",
        "  # Calculate the elapsed time\n",
        "  inf_elapsed_time = inf_end_time - inf_start_time\n",
        "  print(\"Inference time elapsed: \",inf_elapsed_time)\n",
        "  return inf_elapsed_time\n",
        "\n",
        "\n",
        "def calculate_inference_speed_forXruns(modelName,batchsize,numberOfRuns):\n",
        "  infTime = 0\n",
        "  for i in range(numberOfRuns):\n",
        "    infTime += calculate_model_speed(modelName,batchsize)\n",
        "  return (infTime/numberOfRuns)\n",
        "\n",
        "#Includes saving time\n",
        "def calculate_model_speedv2(modelclass, modelName, batchsize):\n",
        "    # Define the transformation for your images\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "    # Create a DataLoader for the validation dataset\n",
        "    batch_size = batchsize  # BS\n",
        "    # Test data 10000\n",
        "    test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Check images number\n",
        "    total_images = len(test_loader.dataset)\n",
        "    print(f'Total number of images in the test set: {total_images}')\n",
        "\n",
        "    # load model\n",
        "    model = modelclass().to(device=device) # FullConnect() #  TreeConnect()\n",
        "    # Load the pre-trained weights\n",
        "    model.load_state_dict(torch.load(modelName))\n",
        "\n",
        "    # Start inference time\n",
        "    inf_start_time = time.time()\n",
        "\n",
        "    # Lists to store predictions and ground truth labels\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Run inference on the test dataset\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            # Append predictions and labels to the lists\n",
        "            all_predictions.append(predicted.cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    # End inference time\n",
        "    inf_end_time = time.time()\n",
        "\n",
        "    # Calculate the elapsed time\n",
        "    inf_elapsed_time = inf_end_time - inf_start_time\n",
        "    print(\"Inference time elapsed: \", inf_elapsed_time)\n",
        "\n",
        "    return inf_elapsed_time\n",
        "\n",
        "\n",
        "\n",
        "######################################################################################################\n",
        "#                                Run functions here\n",
        "######################################################################################################\n",
        "\n",
        "#TrainModel_AMP_V2(TreeConnect_enhanced_BN,\"TC_automated_MP.pth\")\n",
        "#TrainModel(Random_Sparse_enhanced,'sp_locally_30_.pth') #FullConnect Random_Sparse_enhanced\n",
        "#evalualte_model(TreeConnect_enhanced_BN,'tc_bn_v1.pth',64)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#evalualte_model(TreeConnect_enhanced_BN,'TC_automated_MP.pth',1024)\n",
        "\n",
        "from torch.autograd import profiler\n",
        "\n",
        "\n",
        "# Profile the forward pass\n",
        "with profiler.profile(record_shapes=True, use_cuda=torch.cuda.is_available()) as prof:\n",
        "  TrainModel_AMP_V2(TreeConnect_enhanced_BN,\"TC_automated_MP_v2.pth\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #evalualte_model(TreeConnect_enhanced_BN,'TC_automated_MP.pth',1024)\n",
        "  # rangex = 30\n",
        "  # inf_elapsed_timeT =0\n",
        "  # for i in range(rangex):\n",
        "  #   inf_elapsed_timeT += calculate_model_speed(TreeConnect_enhanced_BN,'TC_automated_MP.pth',1024)\n",
        "  # print(\"inference time is: \", inf_elapsed_timeT/(rangex) )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # load model\n",
        "# architecture = 'treeConnect'\n",
        "# modelx =  TreeConnect_enhanced_BN().to(device=device) # FullConnect # TreeConnect()\n",
        "# # Load the pre-trained weights\n",
        "# modelx.load_state_dict(torch.load('TC_automated_MP.pth'))\n",
        "# print(summary(modelx.to(device=device), input_size=(3, 32, 32)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#calculate_model_speed('fullconnect_model_v4_g.pth',1024)\n",
        "#evalualte_model('treeconnect_model_v4_g.pth',1024) # fullconnect_model_v3  treeconnect_model_v3\n",
        "\n",
        "# architecture = 'treeConnect'\n",
        "# print(\"The average inference time for tree connect on batch size 10000 is: \",calculate_inference_speed_forXruns('treeconnect_model_v6_g.pth',10000,100))\n",
        "\n",
        "\n",
        "# architecture = 'fullConnect'\n",
        "# print(\"The average inference time for fully connected on batch size 16 is: \",calculate_inference_speed_forXruns('fullconnect_model_v6_g.pth',16,100))\n",
        "\n",
        "\n",
        "# architecture = 'treeConnect'\n",
        "# calculate_model_speed('treeconnect_model_v6_g.pth',16)\n",
        "\n",
        "# architecture = 'fullConnect'\n",
        "# calculate_model_speed('fullconnect_model_v6_g.pth',16)\n",
        "\n",
        "\n"
      ]
    }
  ]
}