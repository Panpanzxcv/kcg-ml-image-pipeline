{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# Clone repo and go to repo's directory\n",
    "!git clone https://github.com/kk-digital/kcg-ml-image-pipeline\n",
    "!cd ./kcg-ml-image-pipeline"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TpIpcISpenbs",
    "outputId": "363a81f3-9b0c-4b9b-a839-0f73e8630cd4",
    "ExecuteTime": {
     "end_time": "2023-12-08T09:08:31.179886233Z",
     "start_time": "2023-12-08T09:08:04.745901708Z"
    }
   },
   "id": "TpIpcISpenbs",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Cloning into 'kcg-ml-image-pipeline'...\r\n",
      "remote: Enumerating objects: 11244, done.\u001B[K\r\n",
      "remote: Counting objects: 100% (1440/1440), done.\u001B[K\r\n",
      "remote: Compressing objects: 100% (497/497), done.\u001B[K\r\n",
      "Receiving objects:  32% (3647/11244), 24.27 MiB | 1.15 MiB/s  \rReceiving objects:  20% (2249/11244), 9.42 MiB | 4.09 MiB/sReceiving objects:  32% (3645/11244), 12.19 MiB | 2.11 MiB/sReceiving objects:  32% (3646/11244), 13.29 MiB | 680.00 KiB/sfatal: the remote end hung up unexpectedly\r\n",
      "fatal: early EOF\r\n",
      "fatal: index-pack failed\r\n",
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "/bin/bash: line 0: cd: ./kcg-ml-image-pipeline: No such file or directory\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Install requirements\n",
    "!pip install minio torch==2.0.1 torchinfo==1.8.0"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BhM7tKNRfCsY",
    "outputId": "8514c962-4dbd-4a59-8f89-795c36db706e",
    "ExecuteTime": {
     "end_time": "2023-12-08T09:08:40.694450606Z",
     "start_time": "2023-12-08T09:08:31.186125450Z"
    }
   },
   "id": "BhM7tKNRfCsY",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\r\n",
      "Requirement already satisfied: minio in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (7.1.17)\r\n",
      "Requirement already satisfied: torch==2.0.1 in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (2.0.1)\r\n",
      "Requirement already satisfied: torchinfo==1.8.0 in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (1.8.0)\r\n",
      "Requirement already satisfied: certifi in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from minio) (2023.7.22)\r\n",
      "Requirement already satisfied: urllib3 in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from minio) (2.0.5)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from torch==2.0.1) (11.10.3.66)\r\n",
      "Requirement already satisfied: triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from torch==2.0.1) (2.0.0)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from torch==2.0.1) (2.14.3)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from torch==2.0.1) (11.7.99)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from torch==2.0.1) (11.7.4.91)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from torch==2.0.1) (11.4.0.1)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from torch==2.0.1) (11.7.99)\r\n",
      "Requirement already satisfied: typing-extensions in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from torch==2.0.1) (4.8.0)\r\n",
      "Requirement already satisfied: filelock in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from torch==2.0.1) (3.12.4)\r\n",
      "Requirement already satisfied: sympy in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from torch==2.0.1) (1.12)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from torch==2.0.1) (8.5.0.96)\r\n",
      "Requirement already satisfied: networkx in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from torch==2.0.1) (3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from torch==2.0.1) (10.9.0.58)\r\n",
      "Requirement already satisfied: jinja2 in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from torch==2.0.1) (3.1.2)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from torch==2.0.1) (11.7.91)\r\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from torch==2.0.1) (10.2.10.91)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from torch==2.0.1) (11.7.101)\r\n",
      "Requirement already satisfied: wheel in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch==2.0.1) (0.41.2)\r\n",
      "Requirement already satisfied: setuptools in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch==2.0.1) (44.0.0)\r\n",
      "Requirement already satisfied: cmake in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch==2.0.1) (3.27.6)\r\n",
      "Requirement already satisfied: lit in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch==2.0.1) (17.0.1)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from sympy->torch==2.0.1) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /devbox/github-repos/kcg-ml-image-pipeline/env/lib/python3.8/site-packages (from jinja2->torch==2.0.1) (2.1.3)\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Get minio client\n",
    "MINIO_ADDRESS = \"123.176.98.90:9000\"\n",
    "access_key = \"GXvqLWtthELCaROPITOG\"\n",
    "secret_key = \"DmlKgey5u0DnMHP30Vg7rkLT0NNbNIGaM8IwPckD\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "sys.path.append(os.path.abspath('./kcg-ml-image-pipeline/'))\n",
    "from training_worker.ab_ranking.model.ab_ranking_linear import ABRankingModel as ABRankingLinearModel\n",
    "from utility.minio.cmd import is_minio_server_accessible, connect_to_minio_client, download_from_minio, get_file_from_minio\n",
    "\n",
    "minio_client = connect_to_minio_client(MINIO_ADDRESS, access_key, secret_key)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FASOesaZhl6h",
    "outputId": "4ce8dde5-39fb-4731-eaaf-f0cca19cb7e3",
    "ExecuteTime": {
     "end_time": "2023-12-08T09:08:44.466105971Z",
     "start_time": "2023-12-08T09:08:40.703264314Z"
    }
   },
   "id": "FASOesaZhl6h",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to minio client...\n",
      "Successfully connected to minio client...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# get linear positive embedding model\n",
    "linear_model_path = \"environmental/models/ranking/2023-12-08-00-score-linear-embedding-positive.pth\"\n",
    "\n",
    "\n",
    "model_file_data = get_file_from_minio(minio_client, 'datasets', linear_model_path)\n",
    "if not model_file_data:\n",
    "    raise Exception(\"No model file found at path: \", linear_model_path)\n",
    "\n",
    "byte_buffer = io.BytesIO()\n",
    "for data in model_file_data.stream(amt=8192):\n",
    "    byte_buffer.write(data)\n",
    "byte_buffer.seek(0)\n",
    "\n",
    "linear_model = ABRankingLinearModel(inputs_shape=768)\n",
    "model = torch.load(byte_buffer, map_location=torch.device('cpu'))\n",
    "linear_model.model.load_state_dict(model['model_dict'])\n"
   ],
   "metadata": {
    "id": "BLzlCnrsh-r6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a49a9fab-bb1c-4564-ffc4-97271caeb9b9",
    "ExecuteTime": {
     "end_time": "2023-12-08T09:08:46.850150587Z",
     "start_time": "2023-12-08T09:08:44.474914489Z"
    }
   },
   "id": "BLzlCnrsh-r6",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Get CLIP model\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "from configs import model_config\n",
    "from utility.utils_logger import logger\n",
    "from utility.labml.monit import section\n",
    "from utility.minio.cmd import is_minio_server_accessible, connect_to_minio_client, download_from_minio\n",
    "\n",
    "# create dirs\n",
    "if not os.path.exists(\"input/model\"):\n",
    "  os.makedirs(\"input/model\")\n",
    "if not os.path.exists(\"./output\"):\n",
    "  os.makedirs(\"output\")\n",
    "\n",
    "config = model_config.ModelPathConfig()\n",
    "\n",
    "def create_directory_tree_folders(config):\n",
    "    config.create_paths()\n",
    "\n",
    "with section(\"Creating directory tree folders.\"):\n",
    "    create_directory_tree_folders(config)\n",
    "\n",
    "logger.info(\"Downloading models. This may take a while.\")\n",
    "\n",
    "with section(\"Downloading CLIP model from minio\"):\n",
    "    # DOWNLOAD_BASE_CLIP_MODEL\n",
    "    clip_path = \"./kcg-ml-image-pipeline/input/model/clip/vit-large-patch14/model.safetensors\"\n",
    "\n",
    "    bucket_name = \"models\"\n",
    "    object_name = \"clip-vit-large-patch14/model.safetensors\"\n",
    "    download_from_minio(minio_client, bucket_name, object_name, clip_path)\n",
    "\n",
    "!cp ./kcg-ml-image-pipeline/input/model/clip/vit-large-patch14/model.safetensors ./kcg-ml-image-pipeline/input/model/clip/txt_emb_model\n",
    "!cp -r ./kcg-ml-image-pipeline/input ./"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zCwOKS5wjg34",
    "outputId": "3c8202e0-c3de-4b1c-d748-c35fafbb8c99",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-12-08T09:08:46.873717112Z"
    }
   },
   "id": "zCwOKS5wjg34",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory tree folders.\u001B[32m...[DONE]\u001B[0m\u001B[34m\t2.67ms\u001B[0m                                  \n",
      "\u001B[1;32mINFO: Downloading models. This may take a while.\u001B[0m\n",
      "clip-vit-large-patch14/model.safetensors: |##################--| 1532.00 MB/1631.30 MB  93% [elapsed: 30:34 left: 01:58,  0.84 MB/sec]"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Process CLIP model\n",
    "from stable_diffusion.model.clip_text_embedder import CLIPTextEmbedder\n",
    "\n",
    "# CLIP\n",
    "text_embedder = CLIPTextEmbedder()\n",
    "text_embedder.load_submodels()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m0Goc_DmrTzf",
    "outputId": "27524750-5cb8-4a30-8acb-0b9a98b0686c",
    "is_executing": true
   },
   "id": "m0Goc_DmrTzf",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from utility.clip.clip_text_embedder import tensor_attention_pooling, tensor_max_pooling, tensor_max_abs_pooling\n",
    "\n",
    "# For getting prompt score using linear embedding model\n",
    "def get_prompt_linear_embedding_score(text_embedder, linear_model, prompt):\n",
    "  # get embedding of prompt first\n",
    "  embedding, _, attention_mask = text_embedder.forward_return_all(prompt)\n",
    "  # average using attention mask\n",
    "  average_pooled = tensor_attention_pooling(embedding, attention_mask)\n",
    "\n",
    "  # use linear model\n",
    "  score = linear_model.predict_positive_or_negative_only_pooled(average_pooled)\n",
    "\n",
    "  return score.item()"
   ],
   "metadata": {
    "id": "-MXBRS72IQqs",
    "is_executing": true
   },
   "id": "-MXBRS72IQqs",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OlK9O3IJ1F7i",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Get phrase scores probability\n",
    "filename = \"2023_11_29_phrase_scores_probability_09_environmental_positive.csv\"\n",
    "\n",
    "download_urls = ['https://mega.nz/file/4I42VbTY#iZvzE8gc_vPbhvA8YEFqyIYToUoZqSJfvrRLksL4qq8']\n",
    "\n",
    "# Destination paths for data transfer (can be in Google Drive or session storage)\n",
    "destination_paths = ['./']\n",
    "\n",
    "# Mega user name and password. Leave as empty string for anonymous login\n",
    "# note: not used\n",
    "mega_user_email = ''\n",
    "mega_user_password = ''"
   ],
   "id": "OlK9O3IJ1F7i"
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install mega.py"
   ],
   "metadata": {
    "id": "ovxMJBMYiQdq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "aced7323-ab41-4352-dffe-288e36b0d35b",
    "is_executing": true
   },
   "id": "ovxMJBMYiQdq",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1KjTHWu03RA",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "945b6ea7-fe8b-45bf-b037-f8e82b0505dd",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mega import Mega\n",
    "\n",
    "# Create Mega User Object and Login\n",
    "mega = Mega()\n",
    "# mega_user = mega.login(mega_user_email, mega_user_password)\n",
    "\n",
    "\n",
    "# Download files specified in download_urls to locations specified in destination_paths\n",
    "for url, path in zip(download_urls, destination_paths):\n",
    "    # Construct the full path to the file\n",
    "    full_file_path = os.path.join(path, filename)\n",
    "\n",
    "    # Check if file already exists\n",
    "    if os.path.exists(full_file_path):\n",
    "        print(f\"File already exists: {full_file_path}\")\n",
    "    else:\n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "        # Download file\n",
    "        print(f\"Downloading to {path}\")\n",
    "        mega.download_url(url, path)\n",
    "        print(f\"Download complete: {path}\")\n"
   ],
   "id": "w1KjTHWu03RA"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61d7e1e5-1435-4932-9493-181a35411cce",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import zipfile\n",
    "\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot"
   ],
   "id": "61d7e1e5-1435-4932-9493-181a35411cce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUPr1Kwm02mo",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "803200b5-66b3-4604-fb2e-0275e7fc00d4",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "!stat 2023_11_29_phrase_scores_probability_09_environmental_positive.csv"
   ],
   "id": "PUPr1Kwm02mo"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a69e372-9ff2-4e4c-a203-50b246753ca2"
   },
   "source": [
    "# load prompt phrase scores"
   ],
   "id": "4a69e372-9ff2-4e4c-a203-50b246753ca2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29117ed9-dd71-4a1f-bb3a-41d1aac6a356",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "outputId": "53374d96-c225-44cb-d529-79a63b7714dc",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#f = zipfile.ZipFile('./tmp/2023-12-04-phrase-scores-probability-environmental.zip')\n",
    "\n",
    "#positive_phrase_df = pd.read_csv(f.open('2023_11_29_phrase_scores_probability_09_environmental_positive.csv'))\n",
    "positive_phrase_df = pd.read_csv('2023_11_29_phrase_scores_probability_09_environmental_positive.csv')\n",
    "positive_phrase_df"
   ],
   "id": "29117ed9-dd71-4a1f-bb3a-41d1aac6a356"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f191a68b-4970-4d97-ad0f-9f3c6900ce28"
   },
   "source": [
    "# histogram of score"
   ],
   "id": "f191a68b-4970-4d97-ad0f-9f3c6900ce28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6e1f26b8-2b0a-4ace-8b58-a831926cb98a",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "score = positive_phrase_df[\"score\"].values\n",
    "\n",
    "# negate the score\n",
    "score = score * -1"
   ],
   "id": "6e1f26b8-2b0a-4ace-8b58-a831926cb98a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFTegvoI5LYX",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "outputId": "e2bb24f3-5a2a-496c-88b2-93cc3df0e3b0",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(12, 4))\n",
    "\n",
    "pyplot.subplot(1, 2, 1)\n",
    "\n",
    "_ = pyplot.hist(score, bins=1024, log=False)\n",
    "pyplot.title('histogram of score')\n",
    "\n",
    "pyplot.subplot(1, 2, 2)\n",
    "\n",
    "_ = pyplot.hist(score, bins=1024, log=True)\n",
    "pyplot.title('histogram of score (log scale)')"
   ],
   "id": "PFTegvoI5LYX"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d1fbd8f-866d-48b7-b0b4-90e0db47787b"
   },
   "source": [
    "# normal test"
   ],
   "id": "5d1fbd8f-866d-48b7-b0b4-90e0db47787b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sP90knap5ZME",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8aaff123-3c3c-45d7-da25-280f6fdcdb7d",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "score_mean = np.mean(score)\n",
    "score_std = np.std(score)\n",
    "\n",
    "print(\"score_mean= \", score_mean)\n",
    "print(\"score_std= \", score_std)"
   ],
   "id": "sP90knap5ZME"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6812d731-fa93-4fd8-a531-45f1939b0ef5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a0efc146-5407-46d3-9ed3-7e7fd498ff7d",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "stats.kstest(score, 'norm', args=(np.mean(score), np.std(score)))"
   ],
   "id": "6812d731-fa93-4fd8-a531-45f1939b0ef5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# set probability formula"
   ],
   "metadata": {
    "id": "GVI2wCje1NFT"
   },
   "id": "GVI2wCje1NFT"
  },
  {
   "cell_type": "code",
   "source": [
    "boltzman_k = 1.0\n",
    "\n",
    "def probablity_formula(score: np.ndarray, boltzman_temperature: float):\n",
    "\n",
    "  p_array = np.exp(-(score / (boltzman_k * boltzman_temperature)))\n",
    "  p_array_nomralized = p_array  / p_array.sum()  #np.linalg.norm(p_array )\n",
    "\n",
    "  return p_array, p_array_nomralized"
   ],
   "metadata": {
    "id": "RGdgCbQU1MY1",
    "is_executing": true
   },
   "id": "RGdgCbQU1MY1",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29081ced-0fb0-41c5-b7ac-ec03e5bb33e5"
   },
   "source": [
    "# trend of probability with temperature"
   ],
   "id": "29081ced-0fb0-41c5-b7ac-ec03e5bb33e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2779c929-17a5-432c-a602-0f479009aec2",
    "is_executing": true
   },
   "outputs": [],
   "source": [],
   "id": "2779c929-17a5-432c-a602-0f479009aec2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3YZedbZgbX3T",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "outputId": "a94e3e33-c9e8-4c3b-ec8c-01d2f869c7ab",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#raw outputs\n",
    "\n",
    "boltzman_temperatures = list()\n",
    "p_normalized_maxs = list()\n",
    "p_normalized_mins = list()\n",
    "\n",
    "for n in range(1,32):\n",
    "  boltzman_temperature = float(n)\n",
    "\n",
    "  p_array, p_array_nomralized = probablity_formula(score, boltzman_temperature)\n",
    "\n",
    "  p_max = np.max(p_array)\n",
    "  p_min = np.min(p_array)\n",
    "\n",
    "  # print(\"n=%i, min_max_ratio= %.2f, probability_min= %.6f, probability_max= %.6f\" % (n, p_max/p_min, p_min, p_max))\n",
    "\n",
    "  p_normalized_max = np.max(p_array_nomralized)\n",
    "  p_normalized_min = np.min(p_array_nomralized)\n",
    "\n",
    "  # print(\"n=%i, normalized_min_max_ratio= %.2f, normalized_probability_min= %.6f, normalized_probability_max= %.6f\" % (n, p_normalized_max/p_normalized_min, p_normalized_min, p_normalized_max))\n",
    "\n",
    "  boltzman_temperatures.append(boltzman_temperature)\n",
    "  p_normalized_maxs.append(p_normalized_max)\n",
    "  p_normalized_mins.append(p_normalized_min)\n",
    "\n",
    "\n",
    "pyplot.figure(figsize=(12, 4))\n",
    "\n",
    "pyplot.subplot(1, 2, 1)\n",
    "\n",
    "pyplot.plot(boltzman_temperatures, p_normalized_maxs, color='red', label='p_normalized_max')\n",
    "pyplot.plot(boltzman_temperatures, p_normalized_mins, color='blue', label='p_normalized_min')\n",
    "pyplot.legend()\n",
    "pyplot.title(f'trend of normalized probability (linear scale)')\n",
    "\n",
    "pyplot.subplot(1, 2, 2)\n",
    "\n",
    "pyplot.semilogy(boltzman_temperatures, p_normalized_maxs, color='red', label='p_normalized_max')\n",
    "pyplot.semilogy(boltzman_temperatures, p_normalized_mins, color='blue', label='p_normalized_min')\n",
    "pyplot.legend()\n",
    "pyplot.title(f'trend of normalized probability (log scale)')\n"
   ],
   "id": "3YZedbZgbX3T"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvd6idVizPoc"
   },
   "source": [
    "Note:\n",
    "- As temperature increases, selection probability goes to uniform\n",
    "-"
   ],
   "id": "wvd6idVizPoc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# specific temperature and analysis"
   ],
   "metadata": {
    "id": "b5BehH_mz0ve"
   },
   "id": "b5BehH_mz0ve"
  },
  {
   "cell_type": "code",
   "source": [
    "boltzman_temperature = 8\n",
    "\n",
    "p_array, p_array_nomralized = probablity_formula(score, boltzman_temperature)"
   ],
   "metadata": {
    "id": "Csf-7xWWzvas",
    "is_executing": true
   },
   "id": "Csf-7xWWzvas",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## histogram of normalized probability"
   ],
   "metadata": {
    "id": "JK0wsXzK0I3T"
   },
   "id": "JK0wsXzK0I3T"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "249b9efd-64df-4398-9061-a21bb6d27eb3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "outputId": "e8025955-a18f-4e50-cd15-e46e1f172d19",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(12, 4))\n",
    "\n",
    "pyplot.subplot(1, 2, 1)\n",
    "\n",
    "_ = pyplot.hist(p_array_nomralized, bins=256, log=False)\n",
    "pyplot.title(f'hist: normalized probability, temperature {boltzman_temperature} (linear scale)')\n",
    "\n",
    "pyplot.subplot(1, 2, 2)\n",
    "\n",
    "_ = pyplot.hist(p_array_nomralized, bins=256, log=True)\n",
    "pyplot.title(f'hist: normalized probability, temperature {boltzman_temperature} (log scale)')"
   ],
   "id": "249b9efd-64df-4398-9061-a21bb6d27eb3"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HjmUagp8Yfb"
   },
   "source": [
    "## histogram of cumulative probability"
   ],
   "id": "1HjmUagp8Yfb"
  },
  {
   "cell_type": "code",
   "source": [
    "def get_normalized_probability(scores,\n",
    "                              boltzman_k,\n",
    "                              boltzman_temperature):\n",
    "  probability = np.exp(-(score / (boltzman_k * boltzman_temperature)))\n",
    "  #normalized_probability = probability / np.linalg.norm(probability) #probability.sum()\n",
    "  normalized_probability = probability / probability.sum()\n",
    "\n",
    "  p_max = np.max(probability)\n",
    "  p_min = np.min(probability)\n",
    "\n",
    "  print(\"-----------------------------------------------\")\n",
    "  print(\"boltzman_k = \", boltzman_k)\n",
    "  print(\"boltzman_temperature = \", boltzman_temperature)\n",
    "  print(\"probability_max= \", p_max)\n",
    "  print(\"probability_min= \", p_min)\n",
    "  # _ = pyplot.hist(normalized_probability, bins=256, log=True)\n",
    "  # pyplot.title(f'histogram of normalized probability, temperature {boltzman_temperature} (log scale)')\n",
    "\n",
    "  return normalized_probability"
   ],
   "metadata": {
    "id": "oHX40yxinkk6",
    "is_executing": true
   },
   "id": "oHX40yxinkk6",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_cumulative_probability(normalized_probability):\n",
    "  # we need the sorted indexes to get correct index of chosen phrase\n",
    "  sorted_normalized_probability = []\n",
    "  sorted_indexes = sorted(range(len(normalized_probability)), key=lambda x: normalized_probability[x],\n",
    "                                  reverse=True)\n",
    "  for i in sorted_indexes:\n",
    "      sorted_normalized_probability.append(normalized_probability[i])\n",
    "  sorted_normalized_probability = np.array(sorted_normalized_probability)\n",
    "\n",
    "  # sorted_normalized_probability = np.array(sorted(normalized_probability, reverse=True))\n",
    "  cumulative_probability = sorted_normalized_probability.cumsum()\n",
    "\n",
    "  # assert cumulative_probability.sum() == 1.0, \"Error: culmulative probability does not add to 1.0\"\n",
    "\n",
    "  return sorted_indexes, cumulative_probability"
   ],
   "metadata": {
    "id": "ozCCDleimQnV",
    "is_executing": true
   },
   "id": "ozCCDleimQnV",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4b5c4487-c4da-4cf0-ac52-c15f19cd7f7c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b25d473b-039a-40f5-e7cf-84456a77f38c",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "boltzman_temperature = 2.0\n",
    "normalized_probability = get_normalized_probability(score,\n",
    "                                                    boltzman_k,\n",
    "                                                    boltzman_temperature)\n",
    "sorted_indexes, cumulative_probability = get_cumulative_probability(normalized_probability)\n"
   ],
   "id": "4b5c4487-c4da-4cf0-ac52-c15f19cd7f7c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lp0twj0c8WhY",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ],
   "id": "Lp0twj0c8WhY"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9e2e553a-c037-4a5f-8b75-fa200f4c96f7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "outputId": "4e0b35ee-5fef-4d47-e133-f1ab65eb46f1",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "### TODO:\n",
    "### Assert than cumulative probability range is 0.00 to 1.00\n",
    "#_ = pyplot.hist(cumulative_probability, bins=256, log=True)\n",
    "#pyplot.title(f'histogram of cumulative probability, temperature {boltzman_temperature} (log scale)')\n",
    "\n",
    "cumulative_probability_log2 = np.log2(cumulative_probability)\n",
    "\n",
    "pyplot.title(f'CDF, T={boltzman_temperature} (log2 scale)')\n",
    "pyplot.plot(cumulative_probability_log2, color='red', label='CDF_log2')\n",
    "#pyplot.plot(boltzman_temperatures, p_normalized_mins, color='blue', label='p_normalized_min')\n",
    "pyplot.legend()\n"
   ],
   "id": "9e2e553a-c037-4a5f-8b75-fa200f4c96f7"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n87R5i6kvwHJ"
   },
   "source": [
    "Sampling"
   ],
   "id": "n87R5i6kvwHJ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lW0HJ7bBvyeg",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#Add function\n",
    "#uses cumulative probability distribution\n",
    "#choose random number between 0.00 and 1.00\n",
    "#use to choose next prompt from list\n",
    "#keep adding phrases/prompts until token length would go over 75 (using token length parameter in csv)\n",
    "\n",
    "#NOTE: make sure we dont use same prompt phrase twice in same generation\n",
    "\n",
    "import math\n",
    "\n",
    "# find the first element, whose cumulative prob is more than the random float\n",
    "def find_first_element_binary_search(cumulative_prob_arr, random_float):\n",
    "    low = 0\n",
    "    high = len(cumulative_prob_arr) - 1\n",
    "    mid = 0\n",
    "\n",
    "    loop_count = 0\n",
    "    while low < high:\n",
    "        loop_count += 1\n",
    "        assert loop_count < 32, \"Error: binary search loop count is more than 32\"\n",
    "\n",
    "        mid = (high + low) / 2\n",
    "        mid = math.floor(mid)\n",
    "\n",
    "        # If random_float is greater, ignore left half\n",
    "        if cumulative_prob_arr[mid] < random_float:\n",
    "            low = mid + 1\n",
    "        # If random_float is smaller, ignore right half\n",
    "        elif cumulative_prob_arr[mid] >= random_float:\n",
    "            high = mid - 1\n",
    "\n",
    "        # use this index since sometimes the exact\n",
    "        # random num is not in the list\n",
    "        if low == high:\n",
    "            # assert cumulative_prob_arr[low-1] < random_float\n",
    "            # assert cumulative_prob_arr[low] >= random_float, \"{} >= {}, next index val={}\".format(cumulative_prob_arr[low], random_float, cumulative_prob_arr[low+1])\n",
    "            # assert round(cumulative_prob_arr[low], 4) >= 0.0, \"val={}\".format(cumulative_prob_arr[low])\n",
    "            # assert round(cumulative_prob_arr[low], 4) <= 1.0, \"val={}\".format(cumulative_prob_arr[low])\n",
    "\n",
    "            return low\n",
    "\n",
    "\n",
    "    # If we reach here, then the element was not present\n",
    "    return -1"
   ],
   "id": "lW0HJ7bBvyeg"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CO8_3Z4sX59a",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_prompt(positive_phrase_df,\n",
    "                    positive_phrase_origin_indexes,\n",
    "                    positive_cumulative_probability_arr,\n",
    "                    ):\n",
    "    max_token_size = 75\n",
    "    comma_token_size = 1\n",
    "\n",
    "    positive_prompt_total_token_size = 0\n",
    "    positive_prompt = []\n",
    "    positive_prompt_indexes = []\n",
    "    positive_used_phrase_dict = {}\n",
    "\n",
    "    positive_cumulative_probability_arr_min = positive_cumulative_probability_arr.min()\n",
    "    positive_cumulative_probability_arr_max = positive_cumulative_probability_arr.max()\n",
    "    # positive prompt\n",
    "    while positive_prompt_total_token_size < max_token_size:\n",
    "        random_float = random.uniform(positive_cumulative_probability_arr_min,\n",
    "                                      positive_cumulative_probability_arr_max)\n",
    "        random_index = find_first_element_binary_search(positive_cumulative_probability_arr, random_float)\n",
    "        if random_index in positive_used_phrase_dict:\n",
    "            continue\n",
    "\n",
    "        prompt_index = positive_phrase_origin_indexes[random_index]\n",
    "        random_phrase = positive_phrase_df[\"phrase\"][prompt_index]\n",
    "\n",
    "        chosen_phrase_size = positive_phrase_df[\"token length\"][prompt_index]\n",
    "        sum_token_size = positive_prompt_total_token_size + chosen_phrase_size + comma_token_size\n",
    "        if sum_token_size < max_token_size:\n",
    "            # update used array\n",
    "            positive_used_phrase_dict[random_index] = 1\n",
    "            positive_prompt.append(str(random_phrase))\n",
    "            positive_prompt_indexes.append(prompt_index)\n",
    "            positive_prompt_total_token_size = sum_token_size\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    positive_prompt_str = ', '.join([prompt for prompt in positive_prompt])\n",
    "\n",
    "    return positive_prompt_str, positive_prompt_indexes"
   ],
   "id": "CO8_3Z4sX59a"
  },
  {
   "cell_type": "code",
   "source": [
    "def get_variance(energies, mean):\n",
    "  energies = np.array(energies)\n",
    "  diff = energies - mean\n",
    "  product = diff * diff\n",
    "  sum = product.sum()\n",
    "  variance = sum/len(energies)\n",
    "\n",
    "  return variance"
   ],
   "metadata": {
    "id": "VPKyMtUFP08q",
    "is_executing": true
   },
   "id": "VPKyMtUFP08q",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYzVwf6efNnW",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def get_prompt_score(phrase_scores_arr, prompt_indexes):\n",
    "  score = 0\n",
    "  for index in prompt_indexes:\n",
    "    score += phrase_scores_arr[index]\n",
    "\n",
    "  return score"
   ],
   "id": "cYzVwf6efNnW"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test generate prompts"
   ],
   "metadata": {
    "id": "FYMKGypcC8WX"
   },
   "id": "FYMKGypcC8WX"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rknHsnaVa9KJ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6ec1efc4-f684-4634-d4d9-0b33a961a03a",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# generate N prompts\n",
    "prompts = []\n",
    "prompt_indexes_arr = []\n",
    "prompt_count = 5\n",
    "for i in range(prompt_count):\n",
    "  prompt, prompt_indexes = generate_prompt(positive_phrase_df,\n",
    "                                           sorted_indexes,\n",
    "                                           cumulative_probability)\n",
    "  prompts.append(prompt)\n",
    "  prompt_indexes_arr.append(prompt_indexes)\n",
    "\n",
    "for i in range(len(prompts)):\n",
    "  #Then add up \"score\" for each prompt phrase, to get the total score estimate for whole prompt\n",
    "  prompt_score = get_prompt_score(score, prompt_indexes_arr[i])\n",
    "  print(\"[{}]: Score = {}, Prompt = {}\".format(i, prompt_score, prompts[i]))\n",
    "\n"
   ],
   "id": "rknHsnaVa9KJ"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgMBqe9R9fKy"
   },
   "source": [
    "Graph average prompt energy per temperature"
   ],
   "id": "sgMBqe9R9fKy"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SvAqr7OwIEx",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 640
    },
    "outputId": "fe77e136-70db-4a96-f7b7-165b52c3a69d",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# generate N prompts\n",
    "prompt_count = 1024\n",
    "\n",
    "average_linear_embedding_scores_per_temp = []\n",
    "average_energy_per_temp = []\n",
    "variance_per_temp = []\n",
    "temperature_range = [i for i in range(4,33)]\n",
    "print(\"temperature_range=\", temperature_range)\n",
    "for temperature in tqdm(temperature_range):\n",
    "  prompt_indexes_arr = []\n",
    "  prompt_str_arr = []\n",
    "  normalized_probability = get_normalized_probability(score,\n",
    "                                                      boltzman_k,\n",
    "                                                      temperature)\n",
    "  sorted_indexes, cumulative_probability = get_cumulative_probability(normalized_probability)\n",
    "\n",
    "  # generate prompt\n",
    "  for i in range(prompt_count):\n",
    "    prompt, prompt_indexes = generate_prompt(positive_phrase_df,\n",
    "                                            sorted_indexes,\n",
    "                                            cumulative_probability)\n",
    "    prompt_indexes_arr.append(prompt_indexes)\n",
    "    prompt_str_arr.append(prompt)\n",
    "\n",
    "  prompt_energies = []\n",
    "  prompt_linear_embedding_scores = []\n",
    "  for i in range(len(prompt_indexes_arr)):\n",
    "    #Then add up \"score\" for each prompt phrase, to get the total score estimate for whole prompt\n",
    "    prompt_energy = get_prompt_score(score, prompt_indexes_arr[i])\n",
    "    prompt_energies.append(prompt_energy)\n",
    "\n",
    "    # get linear embedding score\n",
    "    linear_embedding_score = get_prompt_linear_embedding_score(text_embedder,\n",
    "                                                               linear_model,\n",
    "                                                               prompt_str_arr[i])\n",
    "    prompt_linear_embedding_scores.append(linear_embedding_score)\n",
    "\n",
    "  average_energy = np.mean(prompt_energies)\n",
    "  variance = get_variance(prompt_energies, average_energy)\n",
    "  average_linear_embedding_score = np.mean(prompt_linear_embedding_scores)\n",
    "  print(\"mean energy = \", average_energy)\n",
    "  print(\"variance = \", variance)\n",
    "  print(\"linear embedding mean score = \", average_linear_embedding_score)\n",
    "\n",
    "  average_energy_per_temp.append(average_energy)\n",
    "  variance_per_temp.append(variance)\n",
    "  average_linear_embedding_scores_per_temp.append(average_linear_embedding_score)\n",
    "\n",
    "pyplot.figure(figsize=(16, 4))\n",
    "# mean energy\n",
    "pyplot.subplot(1, 2, 1)\n",
    "_ = pyplot.plot(average_energy_per_temp, color='red', label='Average Energy', zorder=0)\n",
    "pyplot.title(f'Average Prompt Energy vs Temperature')\n",
    "pyplot.xlabel(\"Temperature\")\n",
    "pyplot.ylabel(\"Average Prompt Energy\")\n",
    "pyplot.xticks([i for i in range(len(temperature_range))], labels=temperature_range)\n",
    "pyplot.xticks(rotation=90)\n",
    "pyplot.legend()\n",
    "\n",
    "# variance\n",
    "pyplot.subplot(1, 2, 2)\n",
    "_ = pyplot.plot(variance_per_temp, color='red', label='Variance', zorder=0)\n",
    "pyplot.title(f'Variance vs Temperature')\n",
    "pyplot.xlabel(\"Temperature\")\n",
    "pyplot.ylabel(\"Variance\")\n",
    "pyplot.xticks([i for i in range(len(temperature_range))], labels=temperature_range)\n",
    "pyplot.xticks(rotation=90)\n",
    "pyplot.legend()\n",
    "\n",
    "# linear embedding mean energy\n",
    "pyplot.subplot(1, 2, 3)\n",
    "_ = pyplot.plot(average_linear_embedding_scores_per_temp, color='red', label='Linear Embedding Average Energy', zorder=0)\n",
    "pyplot.title(f'Linear Embedding Average Prompt Energy vs Temperature')\n",
    "pyplot.xlabel(\"Temperature\")\n",
    "pyplot.ylabel(\"Linear Embedding Average Prompt Energy\")\n",
    "pyplot.xticks([i for i in range(len(temperature_range))], labels=temperature_range)\n",
    "pyplot.xticks(rotation=90)\n",
    "pyplot.legend()"
   ],
   "id": "2SvAqr7OwIEx"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DUOBT3s8_e5"
   },
   "source": [
    "TODO: Max Entropy Sampling\n",
    "- problem: assign probabilites\n",
    "- solution:\n",
    "-- assign probabilites so that that\n",
    "- sum(P(x)*score(x)) = EnergyTarget\n",
    "- and subject to MAX sum(p(x)*ln(P(x)); maximum entropy\n",
    "-- example: choose probabilities so that they average to EnergyTarget\n",
    "\n",
    "Then sample from the probability distribution sequentially\n",
    "\n"
   ],
   "id": "1DUOBT3s8_e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9dqAuDN-A1D",
    "is_executing": true
   },
   "outputs": [],
   "source": [],
   "id": "c9dqAuDN-A1D"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
